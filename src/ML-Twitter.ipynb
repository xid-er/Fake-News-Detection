{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80dcdfe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "799405c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a920602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from spacymoji import Emoji\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32bf1067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_src_text(row):\n",
    "    path = \"data/en/\" + row['event'] + \"/\" + str(row['threadid']) + \"/source-tweets/\" + str(row['tweetid']) + \".json\"\n",
    "    with open(path, \"r\") as f:\n",
    "        source = json.loads(f.read())\n",
    "        return source['text']\n",
    "\n",
    "def is_true(row):\n",
    "    print(row)\n",
    "    path = \"data/en/\" + row['event'] + \"/\" + str(row['threadid']) + \"/annotation.json\"\n",
    "    with open(path, \"r\") as f:\n",
    "        source = json.loads(f.read())\n",
    "        return str(source.get('true', 'unverified'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "071a37b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event                  putinmissing\n",
      "threadid         577258317942149120\n",
      "tweetid          577258317942149120\n",
      "support                  supporting\n",
      "evidentiality             url-given\n",
      "certainty          somewhat-certain\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/en/putinmissing/577258317942149120/annotation.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Dataset used: https://figshare.com/articles/dataset/PHEME_rumour_scheme_dataset_journalism_use_case/2068650\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/en-scheme-annotations.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreadid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweetid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m}, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_true\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: get_src_text(row), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:9555\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   9544\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m   9546\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m   9547\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   9548\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9553\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   9554\u001b[0m )\n\u001b[1;32m-> 9555\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\apply.py:746\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[1;32m--> 746\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\apply.py:873\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 873\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[0;32m    876\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\apply.py:889\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m    888\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    891\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    892\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    893\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn [4], line 4\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Dataset used: https://figshare.com/articles/dataset/PHEME_rumour_scheme_dataset_journalism_use_case/2068650\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/en-scheme-annotations.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreadid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweetid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m}, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mis_true\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      5\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msrc_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: get_src_text(row), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "Cell \u001b[1;32mIn [3], line 10\u001b[0m, in \u001b[0;36mis_true\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(row)\n\u001b[0;32m      9\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/en/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreadid\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/annotation.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     11\u001b[0m     source \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(f\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(source\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munverified\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/en/putinmissing/577258317942149120/annotation.json'"
     ]
    }
   ],
   "source": [
    "# Dataset used: https://figshare.com/articles/dataset/PHEME_rumour_scheme_dataset_journalism_use_case/2068650\n",
    "df = pd.read_json(\"data/en-scheme-annotations.json\", dtype = {\"threadid\": str, \"tweetid\": str}, lines=True)\n",
    "\n",
    "df['true'] = df.apply(lambda row: is_true(row), axis=1)\n",
    "df['src_text'] = df.apply(lambda row: get_src_text(row), axis=1)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23567ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['src_text'], df['true'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3)\n",
    "\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)\n",
    "\n",
    "train_frame = pd.DataFrame([train_text, train_labels])\n",
    "val_frame = pd.DataFrame([val_text, val_labels])\n",
    "test_frame = pd.DataFrame([test_text, test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23359868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0             22\n",
      "1             18\n",
      "unverified     5\n",
      "Name: true, dtype: int64\n",
      "108    PRINCE IN TORONTO TONIGHT:\\n@3RDEYEGIRL tweete...\n",
      "85     Up to 20 held hostage in Sydney Lindt Cafe sie...\n",
      "170    Five hostages have escaped the besieged Lindt ...\n",
      "76     #sydneysiege is over. 2 confirmed dead, #PrayF...\n",
      "286    BREAKING: Three hostages appear to have escape...\n",
      "                             ...                        \n",
      "156    BREAKING  @SkyBusiness: freed hostage borne hi...\n",
      "265    DETAILS: The hostage site is Lindt Chocolat Ca...\n",
      "226    Currently the #FoxNews website has zero, repea...\n",
      "102    Police convoy and helicopters are rushing to s...\n",
      "250    Local media: 3 people appear to escape from Ma...\n",
      "Name: src_text, Length: 207, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(val_labels.value_counts())\n",
    "print(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ed3fc590",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_spacymoji = spacy.load(\"en_core_web_sm\")\n",
    "emoji = Emoji(nlp_spacymoji, merge_spans=True)\n",
    "nlp_spacymoji.add_pipe('emoji', first=True)\n",
    "# tokenised_train = train_frame.apply(lambda row: nlp_spacymoji(row['src_text']))\n",
    "# #tok_train = nlp_spacymoji(train_text)\n",
    "# print(tokenised_train)\n",
    "\n",
    "def spacy_tokeniser(text):\n",
    "    tokens = []\n",
    "    for w in nlp_spacymoji(text):\n",
    "        tokens.append(w.lemma_.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8264fe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features for TF-IDF\n",
    "tfidf = TfidfVectorizer()#tokenizer=spacy_tokeniser)\n",
    "tfidf.fit(train_text)\n",
    "train_features = tfidf.transform(train_text)\n",
    "validation_features = tfidf.transform(val_text)\n",
    "test_features = tfidf.transform(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c4a6dcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1199)\t0.24445295777678855\n",
      "  (0, 1130)\t0.2633723219567455\n",
      "  (0, 1117)\t0.3973731771501845\n",
      "  (0, 1114)\t0.212110091085962\n",
      "  (0, 1110)\t0.10389814156046775\n",
      "  (0, 1094)\t0.1838272529799555\n",
      "  (0, 959)\t0.2633723219567455\n",
      "  (0, 950)\t0.20491728264962708\n",
      "  (0, 864)\t0.19319072690600506\n",
      "  (0, 799)\t0.2633723219567455\n",
      "  (0, 787)\t0.12850499352435185\n",
      "  (0, 776)\t0.2633723219567455\n",
      "  (0, 725)\t0.22061737780894516\n",
      "  (0, 584)\t0.19319072690600506\n",
      "  (0, 577)\t0.13701228024733503\n",
      "  (0, 555)\t0.08130279123838846\n",
      "  (0, 207)\t0.2633723219567455\n",
      "  (0, 156)\t0.2633723219567455\n",
      "  (0, 26)\t0.2633723219567455\n",
      "  (1, 1144)\t0.25764378654454123\n",
      "  (1, 1110)\t0.15511401535306543\n",
      "  (1, 1064)\t0.18994197665972787\n",
      "  (1, 995)\t0.26280630051960463\n",
      "  (1, 882)\t0.39319989537825556\n",
      "  (1, 643)\t0.28842276607929185\n",
      "  :\t:\n",
      "  (205, 507)\t0.22431768606672714\n",
      "  (205, 343)\t0.297348050812346\n",
      "  (205, 288)\t0.297348050812346\n",
      "  (205, 262)\t0.14337508668613588\n",
      "  (205, 237)\t0.22431768606672714\n",
      "  (205, 121)\t0.17604724059817461\n",
      "  (205, 110)\t0.158321791475399\n",
      "  (206, 1110)\t0.12265776439238822\n",
      "  (206, 1064)\t0.15019827942899666\n",
      "  (206, 1000)\t0.24191670223993583\n",
      "  (206, 841)\t0.2504082285488071\n",
      "  (206, 824)\t0.23456100757286077\n",
      "  (206, 745)\t0.3109262565129299\n",
      "  (206, 701)\t0.24191670223993583\n",
      "  (206, 686)\t0.2504082285488071\n",
      "  (206, 653)\t0.28859085301884163\n",
      "  (206, 538)\t0.07689414480635982\n",
      "  (206, 532)\t0.16555145329986673\n",
      "  (206, 443)\t0.18989020058468437\n",
      "  (206, 388)\t0.2727436320428953\n",
      "  (206, 262)\t0.07496110846993671\n",
      "  (206, 204)\t0.3109262565129299\n",
      "  (206, 118)\t0.2727436320428953\n",
      "  (206, 105)\t0.26045157248945755\n",
      "  (206, 67)\t0.3109262565129299\n"
     ]
    }
   ],
   "source": [
    "print(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1892b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_summary(description, true_labels, predictions):\n",
    "  print(\"Evaluation for: \" + description)\n",
    "  print(classification_report(true_labels, predictions,  digits=3, zero_division=0))\n",
    "  print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f5331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9827112a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6222222222222222\n",
      "Evaluation for: SVC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.737     0.636     0.683        22\n",
      "           1      0.538     0.778     0.636        18\n",
      "  unverified      0.000     0.000     0.000         5\n",
      "\n",
      "    accuracy                          0.622        45\n",
      "   macro avg      0.425     0.471     0.440        45\n",
      "weighted avg      0.576     0.622     0.588        45\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[14  8  0]\n",
      " [ 4 14  0]\n",
      " [ 1  4  0]]\n",
      "Evaluation for: SVC test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.833     0.455     0.588        22\n",
      "           1      0.500     0.889     0.640        18\n",
      "  unverified      1.000     0.200     0.333         5\n",
      "\n",
      "    accuracy                          0.600        45\n",
      "   macro avg      0.778     0.514     0.521        45\n",
      "weighted avg      0.719     0.600     0.581        45\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[10 12  0]\n",
      " [ 2 16  0]\n",
      " [ 0  4  1]]\n",
      "0.6222222222222222\n",
      "0.6\n",
      "Evaluation for: LR (TF-IDF)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.737     0.636     0.683        22\n",
      "           1      0.538     0.778     0.636        18\n",
      "  unverified      0.000     0.000     0.000         5\n",
      "\n",
      "    accuracy                          0.622        45\n",
      "   macro avg      0.425     0.471     0.440        45\n",
      "weighted avg      0.576     0.622     0.588        45\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[14  8  0]\n",
      " [ 4 14  0]\n",
      " [ 1  4  0]]\n",
      "Evaluation for: LR (TF-IDF) test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.706     0.545     0.615        22\n",
      "           1      0.536     0.833     0.652        18\n",
      "  unverified      0.000     0.000     0.000         5\n",
      "\n",
      "    accuracy                          0.600        45\n",
      "   macro avg      0.414     0.460     0.423        45\n",
      "weighted avg      0.559     0.600     0.562        45\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[12 10  0]\n",
      " [ 3 15  0]\n",
      " [ 2  3  0]]\n",
      "0.4\n",
      "Evaluation for: Dummy majority\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000        22\n",
      "           1      0.400     1.000     0.571        18\n",
      "  unverified      0.000     0.000     0.000         5\n",
      "\n",
      "    accuracy                          0.400        45\n",
      "   macro avg      0.133     0.333     0.190        45\n",
      "weighted avg      0.160     0.400     0.229        45\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 0 22  0]\n",
      " [ 0 18  0]\n",
      " [ 0  5  0]]\n",
      "Evaluation for: Dummy MF test\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.000     0.000     0.000        22\n",
      "           1      0.400     1.000     0.571        18\n",
      "  unverified      0.000     0.000     0.000         5\n",
      "\n",
      "    accuracy                          0.400        45\n",
      "   macro avg      0.133     0.333     0.190        45\n",
      "weighted avg      0.160     0.400     0.229        45\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[ 0 22  0]\n",
      " [ 0 18  0]\n",
      " [ 0  5  0]]\n"
     ]
    }
   ],
   "source": [
    "# SVC\n",
    "svc = SVC(kernel='rbf')\n",
    "svc_model = svc.fit(train_features, train_labels)\n",
    "print(svc_model.score(validation_features, val_labels))\n",
    "svc_predicted_labels = svc_model.predict(validation_features)\n",
    "\n",
    "evaluation_summary(\"SVC\", val_labels, svc_predicted_labels)\n",
    "svc_test = svc_model.predict(test_features)\n",
    "evaluation_summary(\"SVC test\", test_labels, svc_test)\n",
    "\n",
    "# Logistic Regression with TF-IDF\n",
    "lr_tfidf = LogisticRegression()\n",
    "lr_model_tfidf = lr_tfidf.fit(train_features, train_labels)\n",
    "print(lr_model_tfidf.score(validation_features, val_labels))\n",
    "print(lr_model_tfidf.score(test_features, test_labels))\n",
    "\n",
    "lr_predicted_labels_tfidf = lr_model_tfidf.predict(validation_features)\n",
    "evaluation_summary(\"LR (TF-IDF)\", val_labels, lr_predicted_labels_tfidf)\n",
    "lrtfidf_test = lr_model_tfidf.predict(test_features)\n",
    "evaluation_summary(\"LR (TF-IDF) test\", test_labels, lrtfidf_test)\n",
    "\n",
    "# Dummy Majority\n",
    "dumb = DummyClassifier(strategy='most_frequent')\n",
    "dumb.fit(train_features, train_labels)\n",
    "print(dumb.score(validation_features, val_labels))\n",
    "dumb_validation_predicted_labels = dumb.predict(validation_features)\n",
    "evaluation_summary(\"Dummy majority\", val_labels, dumb_validation_predicted_labels)\n",
    "dumb_test = dumb.predict(test_features)\n",
    "evaluation_summary(\"Dummy MF test\", test_labels, dumb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d8bc638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           127 18   143  161 32   131  177         19          29   172  ...  \\\n",
      "true         1   1    1    1   0    0    0           0  unverified    0  ...   \n",
      "Unnamed 0  NaN   0  NaN  NaN   0  NaN  NaN  unverified           1  NaN  ...   \n",
      "\n",
      "           288  202  162 5    58   69  17          210  223  54   \n",
      "true         1    0    1   1    1    0   0  unverified    0    0  \n",
      "Unnamed 0  NaN  NaN  NaN   1  NaN  NaN   0         NaN  NaN  NaN  \n",
      "\n",
      "[2 rows x 45 columns]\n",
      "['1' '0' '0' '1' '0' '1' '1' '0' '1' '1' '1' '1' '0' '0' '0' '1' '0' '0'\n",
      " '0' 'unverified' '1' '1' '0' '0' '1' '1' '1' '1' '1' '1' '1' '1' '0' '0'\n",
      " '1' '1' '0' '1' '1' '1' '0' '1' '1' '0' '1']\n",
      "127             1\n",
      "18              1\n",
      "143             1\n",
      "161             1\n",
      "32              0\n",
      "131             0\n",
      "177             0\n",
      "19              0\n",
      "29     unverified\n",
      "172             0\n",
      "92              1\n",
      "51              1\n",
      "215             0\n",
      "7               1\n",
      "206             0\n",
      "285             1\n",
      "269             1\n",
      "118             0\n",
      "36              0\n",
      "112    unverified\n",
      "231             0\n",
      "295    unverified\n",
      "37              0\n",
      "126             0\n",
      "272             1\n",
      "164             1\n",
      "2      unverified\n",
      "104             1\n",
      "139             0\n",
      "83              0\n",
      "78              1\n",
      "130             0\n",
      "53              0\n",
      "198             0\n",
      "163             1\n",
      "288             1\n",
      "202             0\n",
      "162             1\n",
      "5               1\n",
      "58              1\n",
      "69              0\n",
      "17              0\n",
      "210    unverified\n",
      "223             0\n",
      "54              0\n",
      "Name: true, dtype: object\n",
      "<class 'pandas.core.series.Series'>\n",
      "                                                  true  \\\n",
      "127  JUST IN: Germanwings plane crashes in southern...   \n",
      "18   Reports: #CharlieHebdo suspects killed http://...   \n",
      "143  Shots fired on parliament hill after a man wal...   \n",
      "161  BREAKING: Live coverage of hostage situation u...   \n",
      "32   Nearly 7k blacks were murdered last yr--almost...   \n",
      "131  BREAKING: 148 passengers were on board #German...   \n",
      "177  Police stand guard as hostages are held in a c...   \n",
      "19   MORE: Official: French terror suspects want to...   \n",
      "29   Prince is playing a secret show in Toronto ton...   \n",
      "172  UPDATE: Hostages fleeing #Sydney cafe held by ...   \n",
      "92   #UPDATE Three men including two brothers ident...   \n",
      "51   Germanwings passenger plane crashes in France:...   \n",
      "215  Reminder: Forget the robbery, forget the minor...   \n",
      "7    Stills from eyewitness video show two #Charlie...   \n",
      "206  Looks like every Charlie Hebdo cartoonist has ...   \n",
      "285  Prime Minister Tony Abbott's statement on the ...   \n",
      "269  Oh. So NOW the #Ferguson police chief says the...   \n",
      "118  What's so frustrating is that now we are talki...   \n",
      "36   The day #Ferguson cops told a dirty, bloody li...   \n",
      "112  Clearly prince is having a show in #toronto @T...   \n",
      "231  BREAKING: 148 feared dead in crashed #Germanwi...   \n",
      "295  Unconfirmed reports claim that Michael Essien ...   \n",
      "37   A woman in #Ferguson was shot in head last nig...   \n",
      "126  RT @khjelmgaard: German media reporting #Andre...   \n",
      "272  Watch video showing gunfire inside Canada's pa...   \n",
      "164  We can see people coming out a firedoor near t...   \n",
      "2    Hoppla! @L0gg0l: Swiss Rumors: Putin absence d...   \n",
      "104  Eleven dead in shooting at Paris offices of sa...   \n",
      "139  Reports now that 2nd shooter has been shot in ...   \n",
      "83   Approximately 50 hostages may be held captive ...   \n",
      "78   #BREAKING: At least two people have died follo...   \n",
      "130  Heart goes out to 148 passengers and crew of G...   \n",
      "53   German Wings airline tweeting now about report...   \n",
      "198  Breaking: Gunman at Paris store threatens to k...   \n",
      "163  Several people being held hostage at a Sydney ...   \n",
      "288  Hostage situation erupts in Sydney cafe, Austr...   \n",
      "202  Rural region of northern France searched in hu...   \n",
      "162  BREAKING UPDATE: Gunman says four devices are ...   \n",
      "5    11 confirmed dead, Francois Hollande to visit ...   \n",
      "58   The soldier shot dead in Wednesday's Ottawa at...   \n",
      "69   #Ottawa police confirm to #CBCNews they are st...   \n",
      "17   Witnesses say the Charlie Hebdo gunmen identif...   \n",
      "210  People talking about stupid shit like governme...   \n",
      "223  Possible \"robbery\" took place and BAM! Like ma...   \n",
      "54   Witness tells #CBCNews suspected shooter of un...   \n",
      "\n",
      "                                             predicted  \n",
      "127  JUST IN: Germanwings plane crashes in southern...  \n",
      "18   Reports: #CharlieHebdo suspects killed http://...  \n",
      "143  Shots fired on parliament hill after a man wal...  \n",
      "161  BREAKING: Live coverage of hostage situation u...  \n",
      "32   Nearly 7k blacks were murdered last yr--almost...  \n",
      "131  BREAKING: 148 passengers were on board #German...  \n",
      "177  Police stand guard as hostages are held in a c...  \n",
      "19   MORE: Official: French terror suspects want to...  \n",
      "29   Prince is playing a secret show in Toronto ton...  \n",
      "172  UPDATE: Hostages fleeing #Sydney cafe held by ...  \n",
      "92   #UPDATE Three men including two brothers ident...  \n",
      "51   Germanwings passenger plane crashes in France:...  \n",
      "215  Reminder: Forget the robbery, forget the minor...  \n",
      "7    Stills from eyewitness video show two #Charlie...  \n",
      "206  Looks like every Charlie Hebdo cartoonist has ...  \n",
      "285  Prime Minister Tony Abbott's statement on the ...  \n",
      "269  Oh. So NOW the #Ferguson police chief says the...  \n",
      "118  What's so frustrating is that now we are talki...  \n",
      "36   The day #Ferguson cops told a dirty, bloody li...  \n",
      "112  Clearly prince is having a show in #toronto @T...  \n",
      "231  BREAKING: 148 feared dead in crashed #Germanwi...  \n",
      "295  Unconfirmed reports claim that Michael Essien ...  \n",
      "37   A woman in #Ferguson was shot in head last nig...  \n",
      "126  RT @khjelmgaard: German media reporting #Andre...  \n",
      "272  Watch video showing gunfire inside Canada's pa...  \n",
      "164  We can see people coming out a firedoor near t...  \n",
      "2    Hoppla! @L0gg0l: Swiss Rumors: Putin absence d...  \n",
      "104  Eleven dead in shooting at Paris offices of sa...  \n",
      "139  Reports now that 2nd shooter has been shot in ...  \n",
      "83   Approximately 50 hostages may be held captive ...  \n",
      "78   #BREAKING: At least two people have died follo...  \n",
      "130  Heart goes out to 148 passengers and crew of G...  \n",
      "53   German Wings airline tweeting now about report...  \n",
      "198  Breaking: Gunman at Paris store threatens to k...  \n",
      "163  Several people being held hostage at a Sydney ...  \n",
      "288  Hostage situation erupts in Sydney cafe, Austr...  \n",
      "202  Rural region of northern France searched in hu...  \n",
      "162  BREAKING UPDATE: Gunman says four devices are ...  \n",
      "5    11 confirmed dead, Francois Hollande to visit ...  \n",
      "58   The soldier shot dead in Wednesday's Ottawa at...  \n",
      "69   #Ottawa police confirm to #CBCNews they are st...  \n",
      "17   Witnesses say the Charlie Hebdo gunmen identif...  \n",
      "210  People talking about stupid shit like governme...  \n",
      "223  Possible \"robbery\" took place and BAM! Like ma...  \n",
      "54   Witness tells #CBCNews suspected shooter of un...  \n",
      "           true   predicted\n",
      "127           1           1\n",
      "18            1           0\n",
      "143           1           0\n",
      "161           1           1\n",
      "32            0           0\n",
      "131           0           1\n",
      "177           0           1\n",
      "19            0           0\n",
      "29   unverified           1\n",
      "172           0           1\n",
      "92            1           1\n",
      "51            1           1\n",
      "215           0           0\n",
      "7             1           0\n",
      "206           0           0\n",
      "285           1           1\n",
      "269           1           0\n",
      "118           0           0\n",
      "36            0           0\n",
      "112  unverified  unverified\n",
      "231           0           1\n",
      "295  unverified           1\n",
      "37            0           0\n",
      "126           0           0\n",
      "272           1           1\n",
      "164           1           1\n",
      "2    unverified           1\n",
      "104           1           1\n",
      "139           0           1\n",
      "83            0           1\n",
      "78            1           1\n",
      "130           0           1\n",
      "53            0           0\n",
      "198           0           0\n",
      "163           1           1\n",
      "288           1           1\n",
      "202           0           0\n",
      "162           1           1\n",
      "5             1           1\n",
      "58            1           1\n",
      "69            0           0\n",
      "17            0           1\n",
      "210  unverified           1\n",
      "223           0           0\n",
      "54            0           1\n",
      "127             1\n",
      "18              1\n",
      "143             1\n",
      "161             1\n",
      "32              0\n",
      "131             0\n",
      "177             0\n",
      "19              0\n",
      "29     unverified\n",
      "172             0\n",
      "92              1\n",
      "51              1\n",
      "215             0\n",
      "7               1\n",
      "206             0\n",
      "285             1\n",
      "269             1\n",
      "118             0\n",
      "36              0\n",
      "112    unverified\n",
      "231             0\n",
      "295    unverified\n",
      "37              0\n",
      "126             0\n",
      "272             1\n",
      "164             1\n",
      "2      unverified\n",
      "104             1\n",
      "139             0\n",
      "83              0\n",
      "78              1\n",
      "130             0\n",
      "53              0\n",
      "198             0\n",
      "163             1\n",
      "288             1\n",
      "202             0\n",
      "162             1\n",
      "5               1\n",
      "58              1\n",
      "69              0\n",
      "17              0\n",
      "210    unverified\n",
      "223             0\n",
      "54              0\n",
      "Name: true, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def get_src_text_by_index(row):\n",
    "    return df.iloc[row.index]['src_text']\n",
    "\n",
    "accuracy = pd.DataFrame([test_labels, lrtfidf_test])\n",
    "print(accuracy)\n",
    "print(lrtfidf_test)\n",
    "print(test_labels)\n",
    "print(type(test_labels))\n",
    "test_df = pd.DataFrame(test_labels)\n",
    "test_df['predicted'] = lrtfidf_test\n",
    "print(test_df.apply(lambda row: get_src_text_by_index(row)))\n",
    "print(test_df)\n",
    "#test_df = pd.DataFrame({\"tweet_id\":test_labels[0], \"actual_label\": test_labels[1]})\n",
    "test_labels.add(lrtfidf_test)\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95c12365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0']\n"
     ]
    }
   ],
   "source": [
    "# Get output\n",
    "\n",
    "input_text = \"Watch video showing gunfire inside Canada's pa\"\n",
    "input_df = pd.DataFrame({\"src_text\": [input_text]})\n",
    "input_features = tfidf.transform(input_df)\n",
    "\n",
    "predicted_label = lr_model_tfidf.predict(input_features)\n",
    "\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31e38553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225940\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author_id</th>\n",
       "      <th>source</th>\n",
       "      <th>created_at</th>\n",
       "      <th>edit_history_tweet_ids</th>\n",
       "      <th>lang</th>\n",
       "      <th>description</th>\n",
       "      <th>username</th>\n",
       "      <th>verified</th>\n",
       "      <th>name</th>\n",
       "      <th>location</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>937349434668498944</td>\n",
       "      <td>BREAKING: First NFL Team Declares Bankruptcy O...</td>\n",
       "      <td>4219197432</td>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>2017-12-03T15:54:54.000Z</td>\n",
       "      <td>['937349434668498944']</td>\n",
       "      <td>en</td>\n",
       "      <td>Ofelia. Arizmendez @ deplorable me. I am a pro...</td>\n",
       "      <td>OfeliasHeaven</td>\n",
       "      <td>False</td>\n",
       "      <td>Ofelia Duchess Arizmendez</td>\n",
       "      <td>Sugar Land, TX</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>937379378006282240</td>\n",
       "      <td>BREAKING: First NFL Team Declares Bankruptcy O...</td>\n",
       "      <td>3018973429</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>2017-12-03T17:53:54.000Z</td>\n",
       "      <td>['937379378006282240']</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lorn_cramer</td>\n",
       "      <td>False</td>\n",
       "      <td>Lorn Cramer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>937380068590055425</td>\n",
       "      <td>BREAKING: First NFL Team Declares Bankruptcy O...</td>\n",
       "      <td>3018973429</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>2017-12-03T17:56:38.000Z</td>\n",
       "      <td>['937380068590055425']</td>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lorn_cramer</td>\n",
       "      <td>False</td>\n",
       "      <td>Lorn Cramer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>937429898670600192</td>\n",
       "      <td>BREAKING: First NFL Team Declares Bankruptcy O...</td>\n",
       "      <td>23162382</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>2017-12-03T21:14:39.000Z</td>\n",
       "      <td>['937429898670600192']</td>\n",
       "      <td>en</td>\n",
       "      <td>Happily married conservative Pentecostal woman...</td>\n",
       "      <td>starchaser57</td>\n",
       "      <td>False</td>\n",
       "      <td>ðŸŒ¹Star Chaser ðŸŒ¼ðŸ‡ºðŸ‡¸</td>\n",
       "      <td>Ozarks. Missouri</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>937449906352152576</td>\n",
       "      <td>BREAKING: First NFL Team Declares Bankruptcy O...</td>\n",
       "      <td>1409084934</td>\n",
       "      <td>Twitter Web Client</td>\n",
       "      <td>2017-12-03T22:34:09.000Z</td>\n",
       "      <td>['937449906352152576']</td>\n",
       "      <td>en</td>\n",
       "      <td>Deplorable member of The Silenced Majority.. #...</td>\n",
       "      <td>ThePipeStore</td>\n",
       "      <td>False</td>\n",
       "      <td>Mr. Walker ðŸ‡ºðŸ‡¸</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  id  \\\n",
       "0           0  937349434668498944   \n",
       "1           1  937379378006282240   \n",
       "2           2  937380068590055425   \n",
       "3           3  937429898670600192   \n",
       "4           4  937449906352152576   \n",
       "\n",
       "                                                text   author_id  \\\n",
       "0  BREAKING: First NFL Team Declares Bankruptcy O...  4219197432   \n",
       "1  BREAKING: First NFL Team Declares Bankruptcy O...  3018973429   \n",
       "2  BREAKING: First NFL Team Declares Bankruptcy O...  3018973429   \n",
       "3  BREAKING: First NFL Team Declares Bankruptcy O...    23162382   \n",
       "4  BREAKING: First NFL Team Declares Bankruptcy O...  1409084934   \n",
       "\n",
       "               source                created_at  edit_history_tweet_ids lang  \\\n",
       "0  Twitter Web Client  2017-12-03T15:54:54.000Z  ['937349434668498944']   en   \n",
       "1            Facebook  2017-12-03T17:53:54.000Z  ['937379378006282240']   en   \n",
       "2            Facebook  2017-12-03T17:56:38.000Z  ['937380068590055425']   en   \n",
       "3  Twitter for iPhone  2017-12-03T21:14:39.000Z  ['937429898670600192']   en   \n",
       "4  Twitter Web Client  2017-12-03T22:34:09.000Z  ['937449906352152576']   en   \n",
       "\n",
       "                                         description       username  verified  \\\n",
       "0  Ofelia. Arizmendez @ deplorable me. I am a pro...  OfeliasHeaven     False   \n",
       "1                                                NaN    lorn_cramer     False   \n",
       "2                                                NaN    lorn_cramer     False   \n",
       "3  Happily married conservative Pentecostal woman...   starchaser57     False   \n",
       "4  Deplorable member of The Silenced Majority.. #...   ThePipeStore     False   \n",
       "\n",
       "                        name           location   true  \n",
       "0  Ofelia Duchess Arizmendez     Sugar Land, TX  False  \n",
       "1                Lorn Cramer                NaN  False  \n",
       "2                Lorn Cramer                NaN  False  \n",
       "3           ðŸŒ¹Star Chaser ðŸŒ¼ðŸ‡ºðŸ‡¸  Ozarks. Missouri   False  \n",
       "4              Mr. Walker ðŸ‡ºðŸ‡¸                NaN  False  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New dataset (FakeNewsNet)\n",
    "fake_df = pd.read_csv(\"data/FakeNewsNet/fake.csv\")\n",
    "fake_df['true'] = False\n",
    "real_df = pd.read_csv(\"data/FakeNewsNet/real.csv\")\n",
    "real_df['true'] = True\n",
    "\n",
    "df = pd.concat([fake_df, real_df])\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd8ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ad695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a9154b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb#scrollTo=URn-DWJt5xhP\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "import transformers as ppb\n",
    "model_class, tokenizer_class, bert_model_name = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(bert_model_name)\n",
    "model = model_class.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c53fbc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = df['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e5caaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6078363e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 255424266240 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [29], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(padded))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 4\u001b[0m     last_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1007\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1007\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1014\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1015\u001b[0m     embedding_output,\n\u001b[0;32m   1016\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1024\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1025\u001b[0m )\n\u001b[0;32m   1026\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:231\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    228\u001b[0m         token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 231\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m token_type_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_type_embeddings(token_type_ids)\n\u001b[0;32m    234\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m inputs_embeds \u001b[38;5;241m+\u001b[39m token_type_embeddings\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 255424266240 bytes."
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(np.array(padded))\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids) # ERROR: Tries to get 255 GB of RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7697d632",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09049dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['true']\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ae52961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8277ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5866666666666667"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a380a584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62a4ecba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\karli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\karli\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stopwords_dict = Counter(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a72b49da",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_col = ['threadid','tweetid','support', 'evidentiality', 'certainty', 'event']\n",
    "\n",
    "# Impute null values with None\n",
    "def null_process(feature_df):\n",
    "    col = 'src_text'\n",
    "    feature_df.loc[feature_df[col].isnull(), col] = \"None\"\n",
    "    return feature_df\n",
    "\n",
    "# Removed unused clumns\n",
    "def remove_unused_col(df,column_n=remove_col):\n",
    "    df = df.drop(column_n,axis=1)\n",
    "    return df\n",
    "\n",
    "def clean_dataset(df):\n",
    "    # remove unused column\n",
    "    df = remove_unused_col(df)\n",
    "    #impute null values\n",
    "    df = null_process(df)\n",
    "    return df\n",
    "\n",
    "# Cleaning text from unused characters\n",
    "def clean_text(text):\n",
    "    text = str(text).replace(r'http[\\w:/\\.]+', ' ')  # removing urls\n",
    "    text = str(text).replace(r'[^\\.\\w\\s]', ' ')  # remove everything but characters and punctuation\n",
    "    text = str(text).replace('[^a-zA-Z]', ' ')\n",
    "    text = str(text).replace(r'\\s\\s+', ' ')\n",
    "    text = text.lower().strip()\n",
    "    #text = ' '.join(text)    \n",
    "    return text\n",
    "\n",
    "## Nltk Preprocessing includes:\n",
    "# Stop words, Stemming and Lemmetization\n",
    "def nltk_preprocess(text):\n",
    "    text = clean_text(text)\n",
    "    wordlist = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    text = ' '.join([word for word in wordlist if word not in stopwords_dict])\n",
    "    text = [ps.stem(word) for word in wordlist if not word in stopwords_dict]\n",
    "    text = ' '.join([wnl.lemmatize(word) for word in wordlist if word not in stopwords_dict])\n",
    "    return  text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e355c77b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true</th>\n",
       "      <th>src_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unverified</td>\n",
       "      <td>vladimir putin netralized internal coup maybe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unverified</td>\n",
       "      <td>coup rt jimgeraghty rumor russian military att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unverified</td>\n",
       "      <td>hoppla l0gg0l swiss rumor putin absence due gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unverified</td>\n",
       "      <td>putin reappears tv amid claim unwell threat co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>france 10 people dead shooting hq satirical we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         true                                           src_text\n",
       "0  unverified  vladimir putin netralized internal coup maybe ...\n",
       "1  unverified  coup rt jimgeraghty rumor russian military att...\n",
       "2  unverified  hoppla l0gg0l swiss rumor putin absence due gi...\n",
       "3  unverified  putin reappears tv amid claim unwell threat co...\n",
       "4           1  france 10 people dead shooting hq satirical we..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = clean_dataset(df)\n",
    "df[\"src_text\"] = df.src_text.apply(nltk_preprocess)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc2eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataset - FakeNewsNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd3ac82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165392\n",
      "\n",
      "418379\n",
      "['937349434668498944', '937379378006282240', '937380068590055425', '937384406511005696', '937387493451862016', '937400766024896512', '937406789686980608', '937411332240011266', '937415066810503168', '937427631661768704', '937429898670600192', '937436145004302337', '937438119468699648', '937449906352152576', '937450317142286336', '937451599320027136', '937452013939494912', '937452151227510784', '937453119478423553', '937462176293437443']\n"
     ]
    }
   ],
   "source": [
    "politifact_fake_df = pd.read_csv(\"data/FakeNewsNet-master/dataset/politifact_fake.csv\")\n",
    "\n",
    "pf_fake_tweet_ids = politifact_fake_df['tweet_ids'].str.split('\\t').explode().tolist()\n",
    "print(len(pf_fake_tweet_ids))\n",
    "print()\n",
    "\n",
    "politifact_real_df = pd.read_csv(\"data/FakeNewsNet-master/dataset/politifact_real.csv\")\n",
    "pf_real_tweet_ids = politifact_real_df['tweet_ids'].str.split('\\t').explode().tolist()\n",
    "print(len(pf_real_tweet_ids))\n",
    "\n",
    "# def concat_tweet_ids(row):\n",
    "#     global pf_fake_tweet_ids\n",
    "#     print('tweet_ids' in row)\n",
    "#     if 'tweet_ids' in row and row['tweet_ids'] != \"\":\n",
    "#         pf_fake_tweet_ids = pf_fake_tweet_ids + row['tweet_ids'].split(\"\\t\")\n",
    "    \n",
    "# politifact_fake_df.apply(lambda x: concat_tweet_ids(x))\n",
    "# print(pf_fake_tweet_ids)\n",
    "\n",
    "fake_short = pf_fake_tweet_ids[:20]\n",
    "\n",
    "print(fake_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaf3b1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.environ.get('APIKey')\n",
    "api_key_secret = os.environ.get('APIKeySecret')\n",
    "access_token = os.environ.get('AccessToken')\n",
    "access_token_secret = os.environ.get('AccessTokenSecret')\n",
    "bearer_token = os.environ.get('BearerToken')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7809ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/72505232/feature-extraction-with-tweet-ids\n",
    "api = tweepy.Client(consumer_key=api_key, \n",
    "                       consumer_secret=api_key_secret,\n",
    "                       access_token=access_token, \n",
    "                       access_token_secret=access_token_secret,\n",
    "                       bearer_token=bearer_token, \n",
    "                       wait_on_rate_limit=True,\n",
    "                        )\n",
    "\n",
    "auth = tweepy.OAuth1UserHandler(\n",
    "   api_key, api_key_secret, access_token, access_token_secret\n",
    ")\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "## --------------------------------------------------- IMPORTANT! \n",
    "IDs = fake_short\n",
    "\n",
    "tweets_df = pd.DataFrame()\n",
    "\n",
    "print(\"Total tweets to fetch:\", len(IDs))\n",
    "\n",
    "for counter, tweet_id in enumerate(IDs):\n",
    "    try:\n",
    "        if counter % 50 == 0:\n",
    "            print(\"Tweets analysed so far:\", counter)\n",
    "            \n",
    "        info_tweet = api.get_status(tweet_id, tweet_mode=\"extended\")\n",
    "        \n",
    "        row = pd.DataFrame({'ID': info_tweet.id,\n",
    "                             'Text': info_tweet.full_text,\n",
    "                             'Created at': info_tweet.created_at,\n",
    "                             'User location': info_tweet.user.location,\n",
    "                             'Num Followers': info_tweet.user.followers_count,\n",
    "                             'Num Friends': info_tweet.user.friends_count,\n",
    "                             'Num Favourites': info_tweet.user.favourites_count,\n",
    "                             'User description': info_tweet.user.description,\n",
    "                             'User verified': info_tweet.user.verified,\n",
    "                             'Language': info_tweet.lang}, index=[0])\n",
    "        \n",
    "        tweets_df = pd.concat([tweets_df, row])\n",
    "        tweets_df = tweets_df.reset_index(drop=True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"Total successfuly fetched tweets:\", len(tweets_df))\n",
    "\n",
    "tweets_df.to_csv(\"fake_short.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eabbd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165392\n",
      "True\n",
      "165352\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Experiments with Tweet IDs\n",
    "print(len(pf_fake_tweet_ids))\n",
    "print(np.nan in pf_fake_tweet_ids)\n",
    "pf_fake_tweet_ids = [x for x in pf_fake_tweet_ids if x is not np.nan]\n",
    "print(len(pf_fake_tweet_ids))\n",
    "print(np.nan in pf_fake_tweet_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d1ac6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418379\n",
      "True\n",
      "418164\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(len(pf_real_tweet_ids))\n",
    "print(np.nan in pf_real_tweet_ids)\n",
    "pf_real_tweet_ids = [x for x in pf_real_tweet_ids if x is not np.nan]\n",
    "print(len(pf_real_tweet_ids))\n",
    "print(np.nan in pf_real_tweet_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20662d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake news!\n",
      "Tweets analysed so far: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 542 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets analysed so far: 500\n",
      "Tweets analysed so far: 1000\n",
      "Tweets analysed so far: 1500\n",
      "Tweets analysed so far: 2000\n",
      "Tweets analysed so far: 2500\n",
      "Tweets analysed so far: 3000\n",
      "Tweets analysed so far: 3500\n",
      "Tweets analysed so far: 4000\n",
      "Tweets analysed so far: 4500\n",
      "Tweets analysed so far: 5000\n",
      "Tweets analysed so far: 5500\n",
      "Tweets analysed so far: 6000\n",
      "Tweets analysed so far: 6500\n",
      "Tweets analysed so far: 7000\n",
      "Tweets analysed so far: 7500\n",
      "Tweets analysed so far: 8000\n",
      "Tweets analysed so far: 8500\n",
      "Tweets analysed so far: 9000\n",
      "Tweets analysed so far: 9500\n",
      "Tweets analysed so far: 10000\n",
      "Tweets analysed so far: 10500\n",
      "Tweets analysed so far: 11000\n",
      "Tweets analysed so far: 11500\n",
      "Tweets analysed so far: 12000\n",
      "Tweets analysed so far: 12500\n",
      "Tweets analysed so far: 13000\n",
      "Tweets analysed so far: 13500\n",
      "Tweets analysed so far: 14000\n",
      "Tweets analysed so far: 14500\n",
      "Tweets analysed so far: 15000\n",
      "Tweets analysed so far: 15500\n",
      "Tweets analysed so far: 16000\n",
      "Tweets analysed so far: 16500\n",
      "Tweets analysed so far: 17000\n",
      "Tweets analysed so far: 17500\n",
      "Tweets analysed so far: 18000\n",
      "Tweets analysed so far: 18500\n",
      "Tweets analysed so far: 19000\n",
      "Tweets analysed so far: 19500\n",
      "Tweets analysed so far: 20000\n",
      "Tweets analysed so far: 20500\n",
      "Tweets analysed so far: 21000\n",
      "Tweets analysed so far: 21500\n",
      "Tweets analysed so far: 22000\n",
      "Tweets analysed so far: 22500\n",
      "Tweets analysed so far: 23000\n",
      "Tweets analysed so far: 23500\n",
      "Tweets analysed so far: 24000\n",
      "Tweets analysed so far: 24500\n",
      "Tweets analysed so far: 25000\n",
      "Tweets analysed so far: 25500\n",
      "Tweets analysed so far: 26000\n",
      "Tweets analysed so far: 26500\n",
      "Tweets analysed so far: 27000\n",
      "Tweets analysed so far: 27500\n",
      "Tweets analysed so far: 28000\n",
      "Tweets analysed so far: 28500\n",
      "Tweets analysed so far: 29000\n",
      "Tweets analysed so far: 29500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 762 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets analysed so far: 30000\n",
      "Tweets analysed so far: 30500\n",
      "Tweets analysed so far: 31000\n",
      "Tweets analysed so far: 31500\n",
      "Tweets analysed so far: 32000\n",
      "Tweets analysed so far: 32500\n",
      "Tweets analysed so far: 33000\n",
      "Tweets analysed so far: 33500\n",
      "Tweets analysed so far: 34000\n",
      "Tweets analysed so far: 34500\n",
      "Tweets analysed so far: 35000\n",
      "Tweets analysed so far: 35500\n",
      "Tweets analysed so far: 36000\n",
      "Tweets analysed so far: 36500\n",
      "Tweets analysed so far: 37000\n",
      "Tweets analysed so far: 37500\n",
      "Tweets analysed so far: 38000\n",
      "Tweets analysed so far: 38500\n",
      "Tweets analysed so far: 39000\n",
      "Tweets analysed so far: 39500\n",
      "Tweets analysed so far: 40000\n",
      "Tweets analysed so far: 40500\n",
      "Tweets analysed so far: 41000\n",
      "Tweets analysed so far: 41500\n",
      "Tweets analysed so far: 42000\n",
      "Tweets analysed so far: 42500\n",
      "Tweets analysed so far: 43000\n",
      "Tweets analysed so far: 43500\n",
      "Tweets analysed so far: 44000\n",
      "Tweets analysed so far: 44500\n",
      "Tweets analysed so far: 45000\n",
      "Tweets analysed so far: 45500\n",
      "Tweets analysed so far: 46000\n",
      "Tweets analysed so far: 46500\n",
      "Tweets analysed so far: 47000\n",
      "Tweets analysed so far: 47500\n",
      "Tweets analysed so far: 48000\n",
      "Tweets analysed so far: 48500\n",
      "Tweets analysed so far: 49000\n",
      "Tweets analysed so far: 49500\n",
      "Tweets analysed so far: 50000\n",
      "Tweets analysed so far: 50500\n",
      "Tweets analysed so far: 51000\n",
      "Tweets analysed so far: 51500\n",
      "Tweets analysed so far: 52000\n",
      "Tweets analysed so far: 52500\n",
      "Tweets analysed so far: 53000\n",
      "Tweets analysed so far: 53500\n",
      "Tweets analysed so far: 54000\n",
      "Tweets analysed so far: 54500\n",
      "Tweets analysed so far: 55000\n",
      "Tweets analysed so far: 55500\n",
      "Tweets analysed so far: 56000\n",
      "Tweets analysed so far: 56500\n",
      "Tweets analysed so far: 57000\n",
      "Tweets analysed so far: 57500\n",
      "Tweets analysed so far: 58000\n",
      "Tweets analysed so far: 58500\n",
      "Tweets analysed so far: 59000\n",
      "Tweets analysed so far: 59500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 721 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets analysed so far: 60000\n",
      "Tweets analysed so far: 60500\n",
      "Tweets analysed so far: 61000\n",
      "Tweets analysed so far: 61500\n",
      "Tweets analysed so far: 62000\n",
      "Tweets analysed so far: 62500\n",
      "Tweets analysed so far: 63000\n",
      "Tweets analysed so far: 63500\n",
      "Tweets analysed so far: 64000\n",
      "Tweets analysed so far: 64500\n",
      "Tweets analysed so far: 65000\n",
      "Tweets analysed so far: 65500\n",
      "Tweets analysed so far: 66000\n",
      "Tweets analysed so far: 66500\n",
      "Tweets analysed so far: 67000\n",
      "Tweets analysed so far: 67500\n",
      "Tweets analysed so far: 68000\n",
      "Tweets analysed so far: 68500\n",
      "Tweets analysed so far: 69000\n",
      "Tweets analysed so far: 69500\n",
      "Tweets analysed so far: 70000\n",
      "Tweets analysed so far: 70500\n",
      "Tweets analysed so far: 71000\n",
      "Tweets analysed so far: 71500\n",
      "Tweets analysed so far: 72000\n",
      "Tweets analysed so far: 72500\n",
      "Tweets analysed so far: 73000\n",
      "Tweets analysed so far: 73500\n",
      "Tweets analysed so far: 74000\n",
      "Tweets analysed so far: 74500\n",
      "Tweets analysed so far: 75000\n",
      "Tweets analysed so far: 75500\n",
      "Tweets analysed so far: 76000\n",
      "Tweets analysed so far: 76500\n",
      "Tweets analysed so far: 77000\n",
      "Tweets analysed so far: 77500\n",
      "Tweets analysed so far: 78000\n",
      "Tweets analysed so far: 78500\n",
      "Tweets analysed so far: 79000\n",
      "Tweets analysed so far: 79500\n",
      "Tweets analysed so far: 80000\n",
      "Tweets analysed so far: 80500\n",
      "Tweets analysed so far: 81000\n",
      "Tweets analysed so far: 81500\n",
      "Tweets analysed so far: 82000\n",
      "Tweets analysed so far: 82500\n",
      "Tweets analysed so far: 83000\n",
      "Tweets analysed so far: 83500\n",
      "Tweets analysed so far: 84000\n",
      "Tweets analysed so far: 84500\n",
      "Tweets analysed so far: 85000\n",
      "Tweets analysed so far: 85500\n",
      "Tweets analysed so far: 86000\n",
      "Tweets analysed so far: 86500\n",
      "Tweets analysed so far: 87000\n",
      "Tweets analysed so far: 87500\n",
      "Tweets analysed so far: 88000\n",
      "Tweets analysed so far: 88500\n",
      "Tweets analysed so far: 89000\n",
      "Tweets analysed so far: 89500\n",
      "Tweets analysed so far: 90000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 107 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets analysed so far: 90500\n",
      "Tweets analysed so far: 91000\n",
      "Tweets analysed so far: 91500\n",
      "Tweets analysed so far: 92000\n",
      "Tweets analysed so far: 92500\n",
      "Tweets analysed so far: 93000\n",
      "Tweets analysed so far: 93500\n",
      "Tweets analysed so far: 94000\n",
      "Tweets analysed so far: 94500\n",
      "Tweets analysed so far: 95000\n",
      "Tweets analysed so far: 95500\n",
      "Tweets analysed so far: 96000\n",
      "Tweets analysed so far: 96500\n",
      "Tweets analysed so far: 97000\n",
      "Tweets analysed so far: 97500\n",
      "Tweets analysed so far: 98000\n",
      "Tweets analysed so far: 98500\n",
      "Tweets analysed so far: 99000\n",
      "Tweets analysed so far: 99500\n",
      "Tweets analysed so far: 100000\n",
      "Tweets analysed so far: 100500\n",
      "Tweets analysed so far: 101000\n",
      "Tweets analysed so far: 101500\n",
      "Tweets analysed so far: 102000\n",
      "Tweets analysed so far: 102500\n",
      "Tweets analysed so far: 103000\n",
      "Tweets analysed so far: 103500\n",
      "Tweets analysed so far: 104000\n",
      "Tweets analysed so far: 104500\n",
      "Tweets analysed so far: 105000\n",
      "Tweets analysed so far: 105500\n",
      "Tweets analysed so far: 106000\n",
      "Tweets analysed so far: 106500\n",
      "Tweets analysed so far: 107000\n",
      "Tweets analysed so far: 107500\n",
      "Tweets analysed so far: 108000\n",
      "Tweets analysed so far: 108500\n",
      "Tweets analysed so far: 109000\n",
      "Tweets analysed so far: 109500\n",
      "Tweets analysed so far: 110000\n",
      "Tweets analysed so far: 110500\n",
      "Tweets analysed so far: 111000\n",
      "Tweets analysed so far: 111500\n",
      "Tweets analysed so far: 112000\n",
      "Tweets analysed so far: 112500\n",
      "Tweets analysed so far: 113000\n",
      "Tweets analysed so far: 113500\n",
      "Tweets analysed so far: 114000\n",
      "Tweets analysed so far: 114500\n",
      "Tweets analysed so far: 115000\n",
      "Tweets analysed so far: 115500\n",
      "Tweets analysed so far: 116000\n",
      "Tweets analysed so far: 116500\n",
      "Tweets analysed so far: 117000\n",
      "Tweets analysed so far: 117500\n",
      "Tweets analysed so far: 118000\n",
      "Tweets analysed so far: 118500\n",
      "Tweets analysed so far: 119000\n",
      "Tweets analysed so far: 119500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 764 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets analysed so far: 120000\n",
      "Tweets analysed so far: 120500\n",
      "Tweets analysed so far: 121000\n",
      "Tweets analysed so far: 121500\n",
      "Tweets analysed so far: 122000\n",
      "Tweets analysed so far: 122500\n",
      "Tweets analysed so far: 123000\n",
      "Tweets analysed so far: 123500\n",
      "Tweets analysed so far: 124000\n",
      "Tweets analysed so far: 124500\n",
      "Tweets analysed so far: 125000\n",
      "Tweets analysed so far: 125500\n",
      "Tweets analysed so far: 126000\n",
      "Tweets analysed so far: 126500\n",
      "Tweets analysed so far: 127000\n",
      "Tweets analysed so far: 127500\n",
      "Tweets analysed so far: 128000\n",
      "Tweets analysed so far: 128500\n",
      "Tweets analysed so far: 129000\n",
      "Tweets analysed so far: 129500\n",
      "Tweets analysed so far: 130000\n",
      "Tweets analysed so far: 130500\n",
      "Tweets analysed so far: 131000\n",
      "Tweets analysed so far: 131500\n",
      "Tweets analysed so far: 132000\n",
      "Tweets analysed so far: 132500\n",
      "Tweets analysed so far: 133000\n",
      "Tweets analysed so far: 133500\n",
      "Tweets analysed so far: 134000\n",
      "Tweets analysed so far: 134500\n",
      "Tweets analysed so far: 135000\n",
      "Tweets analysed so far: 135500\n",
      "Tweets analysed so far: 136000\n",
      "Tweets analysed so far: 136500\n",
      "Tweets analysed so far: 137000\n",
      "Tweets analysed so far: 137500\n",
      "Tweets analysed so far: 138000\n",
      "Tweets analysed so far: 138500\n",
      "Tweets analysed so far: 139000\n",
      "Tweets analysed so far: 139500\n",
      "Tweets analysed so far: 140000\n",
      "Tweets analysed so far: 140500\n",
      "Tweets analysed so far: 141000\n",
      "Tweets analysed so far: 141500\n",
      "Tweets analysed so far: 142000\n",
      "Tweets analysed so far: 142500\n",
      "Tweets analysed so far: 143000\n",
      "Tweets analysed so far: 143500\n",
      "Tweets analysed so far: 144000\n",
      "Tweets analysed so far: 144500\n",
      "Tweets analysed so far: 145000\n",
      "Tweets analysed so far: 145500\n",
      "Tweets analysed so far: 146000\n",
      "Tweets analysed so far: 146500\n",
      "Tweets analysed so far: 147000\n",
      "Tweets analysed so far: 147500\n",
      "Tweets analysed so far: 148000\n",
      "Tweets analysed so far: 148500\n",
      "Tweets analysed so far: 149000\n",
      "Tweets analysed so far: 149500\n",
      "Tweets analysed so far: 150000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 760 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets analysed so far: 150500\n",
      "Tweets analysed so far: 151000\n",
      "Tweets analysed so far: 151500\n",
      "Tweets analysed so far: 152000\n",
      "Tweets analysed so far: 152500\n",
      "Tweets analysed so far: 153000\n",
      "Tweets analysed so far: 153500\n",
      "Tweets analysed so far: 154000\n",
      "Tweets analysed so far: 154500\n",
      "Tweets analysed so far: 155000\n",
      "Tweets analysed so far: 155500\n",
      "Tweets analysed so far: 156000\n",
      "Tweets analysed so far: 156500\n",
      "Tweets analysed so far: 157000\n",
      "Tweets analysed so far: 157500\n",
      "Tweets analysed so far: 158000\n",
      "Tweets analysed so far: 158500\n",
      "Tweets analysed so far: 159000\n",
      "Tweets analysed so far: 159500\n",
      "Tweets analysed so far: 160000\n",
      "Tweets analysed so far: 160500\n",
      "Tweets analysed so far: 161000\n",
      "Tweets analysed so far: 161500\n",
      "Tweets analysed so far: 162000\n",
      "Tweets analysed so far: 162500\n",
      "Tweets analysed so far: 163000\n",
      "Tweets analysed so far: 163500\n",
      "Tweets analysed so far: 164000\n",
      "Tweets analysed so far: 164500\n",
      "Tweets analysed so far: 165000\n",
      "------------------------------------------------\n",
      "Real news!\n",
      "Tweets analysed so far: 0\n",
      "Tweets analysed so far: 500\n",
      "Tweets analysed so far: 1000\n",
      "Tweets analysed so far: 1500\n",
      "Tweets analysed so far: 2000\n",
      "Tweets analysed so far: 2500\n",
      "Tweets analysed so far: 3000\n",
      "Tweets analysed so far: 3500\n",
      "Tweets analysed so far: 4000\n",
      "Tweets analysed so far: 4500\n",
      "Tweets analysed so far: 5000\n",
      "Tweets analysed so far: 5500\n",
      "Tweets analysed so far: 6000\n",
      "Tweets analysed so far: 6500\n",
      "Tweets analysed so far: 7000\n",
      "Tweets analysed so far: 7500\n",
      "Tweets analysed so far: 8000\n",
      "Tweets analysed so far: 8500\n",
      "Tweets analysed so far: 9000\n",
      "Tweets analysed so far: 9500\n",
      "Tweets analysed so far: 10000\n",
      "Tweets analysed so far: 10500\n",
      "Tweets analysed so far: 11000\n",
      "Tweets analysed so far: 11500\n",
      "Tweets analysed so far: 12000\n",
      "Tweets analysed so far: 12500\n",
      "Tweets analysed so far: 13000\n",
      "Tweets analysed so far: 13500\n",
      "Tweets analysed so far: 14000\n",
      "Tweets analysed so far: 14500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 749 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets analysed so far: 15000\n",
      "Tweets analysed so far: 15500\n",
      "Tweets analysed so far: 16000\n",
      "Tweets analysed so far: 16500\n",
      "Tweets analysed so far: 17000\n",
      "Tweets analysed so far: 17500\n",
      "Tweets analysed so far: 18000\n",
      "Tweets analysed so far: 18500\n",
      "Tweets analysed so far: 19000\n",
      "Tweets analysed so far: 19500\n",
      "Tweets analysed so far: 20000\n",
      "Tweets analysed so far: 20500\n",
      "Tweets analysed so far: 21000\n",
      "Tweets analysed so far: 21500\n",
      "Tweets analysed so far: 22000\n",
      "Tweets analysed so far: 22500\n",
      "Tweets analysed so far: 23000\n",
      "Tweets analysed so far: 23500\n",
      "Tweets analysed so far: 24000\n",
      "Tweets analysed so far: 24500\n",
      "Tweets analysed so far: 25000\n",
      "Tweets analysed so far: 25500\n",
      "Tweets analysed so far: 26000\n",
      "Tweets analysed so far: 26500\n",
      "Tweets analysed so far: 27000\n",
      "Tweets analysed so far: 27500\n",
      "Tweets analysed so far: 28000\n",
      "Tweets analysed so far: 28500\n",
      "Tweets analysed so far: 29000\n",
      "Tweets analysed so far: 29500\n",
      "Tweets analysed so far: 30000\n",
      "Tweets analysed so far: 30500\n",
      "Tweets analysed so far: 31000\n",
      "Tweets analysed so far: 31500\n",
      "Tweets analysed so far: 32000\n",
      "Tweets analysed so far: 32500\n",
      "Tweets analysed so far: 33000\n",
      "Tweets analysed so far: 33500\n",
      "Tweets analysed so far: 34000\n",
      "Tweets analysed so far: 34500\n",
      "Tweets analysed so far: 35000\n",
      "Tweets analysed so far: 35500\n",
      "Tweets analysed so far: 36000\n",
      "Tweets analysed so far: 36500\n",
      "Tweets analysed so far: 37000\n",
      "Tweets analysed so far: 37500\n",
      "Tweets analysed so far: 38000\n",
      "Tweets analysed so far: 38500\n",
      "Tweets analysed so far: 39000\n",
      "Tweets analysed so far: 39500\n",
      "Tweets analysed so far: 40000\n",
      "Tweets analysed so far: 40500\n",
      "Tweets analysed so far: 41000\n",
      "Tweets analysed so far: 41500\n",
      "Tweets analysed so far: 42000\n",
      "Tweets analysed so far: 42500\n",
      "Tweets analysed so far: 43000\n",
      "Tweets analysed so far: 43500\n",
      "Tweets analysed so far: 44000\n",
      "Tweets analysed so far: 44500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 743 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets analysed so far: 45000\n",
      "Tweets analysed so far: 45500\n",
      "Tweets analysed so far: 46000\n",
      "Tweets analysed so far: 46500\n",
      "Tweets analysed so far: 47000\n",
      "Tweets analysed so far: 47500\n",
      "Tweets analysed so far: 48000\n",
      "Tweets analysed so far: 48500\n",
      "Tweets analysed so far: 49000\n",
      "Tweets analysed so far: 49500\n",
      "Tweets analysed so far: 50000\n",
      "Tweets analysed so far: 50500\n",
      "Tweets analysed so far: 51000\n",
      "Tweets analysed so far: 51500\n",
      "Tweets analysed so far: 52000\n",
      "Tweets analysed so far: 52500\n",
      "Tweets analysed so far: 53000\n",
      "Tweets analysed so far: 53500\n",
      "Tweets analysed so far: 54000\n",
      "Tweets analysed so far: 54500\n",
      "Tweets analysed so far: 55000\n",
      "Tweets analysed so far: 55500\n",
      "Tweets analysed so far: 56000\n",
      "Tweets analysed so far: 56500\n",
      "Tweets analysed so far: 57000\n",
      "Tweets analysed so far: 57500\n",
      "Tweets analysed so far: 58000\n",
      "Tweets analysed so far: 58500\n",
      "Tweets analysed so far: 59000\n",
      "Tweets analysed so far: 59500\n",
      "Tweets analysed so far: 60000\n",
      "Tweets analysed so far: 60500\n",
      "Tweets analysed so far: 61000\n",
      "Tweets analysed so far: 61500\n",
      "Tweets analysed so far: 62000\n",
      "Tweets analysed so far: 62500\n",
      "Tweets analysed so far: 63000\n",
      "Tweets analysed so far: 63500\n",
      "Tweets analysed so far: 64000\n",
      "Tweets analysed so far: 64500\n",
      "Tweets analysed so far: 65000\n",
      "Tweets analysed so far: 65500\n",
      "Tweets analysed so far: 66000\n",
      "Tweets analysed so far: 66500\n",
      "Tweets analysed so far: 67000\n",
      "Tweets analysed so far: 67500\n",
      "Tweets analysed so far: 68000\n",
      "Tweets analysed so far: 68500\n",
      "Tweets analysed so far: 69000\n",
      "Tweets analysed so far: 69500\n",
      "Tweets analysed so far: 70000\n",
      "Tweets analysed so far: 70500\n",
      "Tweets analysed so far: 71000\n",
      "Tweets analysed so far: 71500\n",
      "Tweets analysed so far: 72000\n",
      "Tweets analysed so far: 72500\n",
      "Tweets analysed so far: 73000\n",
      "Tweets analysed so far: 73500\n",
      "Tweets analysed so far: 74000\n",
      "Tweets analysed so far: 74500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 748 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets analysed so far: 75000\n",
      "Tweets analysed so far: 75500\n",
      "Tweets analysed so far: 76000\n",
      "Tweets analysed so far: 76500\n",
      "Tweets analysed so far: 77000\n",
      "Tweets analysed so far: 77500\n",
      "Tweets analysed so far: 78000\n",
      "Tweets analysed so far: 78500\n",
      "Tweets analysed so far: 79000\n",
      "Tweets analysed so far: 79500\n",
      "Tweets analysed so far: 80000\n",
      "Tweets analysed so far: 80500\n",
      "Tweets analysed so far: 81000\n",
      "Tweets analysed so far: 81500\n",
      "Tweets analysed so far: 82000\n",
      "Tweets analysed so far: 82500\n",
      "Tweets analysed so far: 83000\n",
      "Tweets analysed so far: 83500\n",
      "Tweets analysed so far: 84000\n",
      "Tweets analysed so far: 84500\n",
      "Tweets analysed so far: 85000\n",
      "Tweets analysed so far: 85500\n",
      "Tweets analysed so far: 86000\n",
      "Tweets analysed so far: 86500\n",
      "Tweets analysed so far: 87000\n",
      "Tweets analysed so far: 87500\n",
      "Tweets analysed so far: 88000\n",
      "Tweets analysed so far: 88500\n",
      "Tweets analysed so far: 89000\n",
      "Tweets analysed so far: 89500\n",
      "Tweets analysed so far: 90000\n",
      "Tweets analysed so far: 90500\n",
      "Tweets analysed so far: 91000\n",
      "Tweets analysed so far: 91500\n",
      "Tweets analysed so far: 92000\n",
      "Tweets analysed so far: 92500\n",
      "Tweets analysed so far: 93000\n",
      "Tweets analysed so far: 93500\n",
      "Tweets analysed so far: 94000\n",
      "Tweets analysed so far: 94500\n",
      "Tweets analysed so far: 95000\n",
      "Tweets analysed so far: 95500\n",
      "Tweets analysed so far: 96000\n",
      "Tweets analysed so far: 96500\n",
      "Tweets analysed so far: 97000\n",
      "Tweets analysed so far: 97500\n",
      "Tweets analysed so far: 98000\n",
      "Tweets analysed so far: 98500\n",
      "Tweets analysed so far: 99000\n",
      "Tweets analysed so far: 99500\n",
      "Tweets analysed so far: 100000\n",
      "Tweets analysed so far: 100500\n",
      "Tweets analysed so far: 101000\n",
      "Tweets analysed so far: 101500\n",
      "Tweets analysed so far: 102000\n",
      "Tweets analysed so far: 102500\n",
      "Tweets analysed so far: 103000\n",
      "Tweets analysed so far: 103500\n",
      "Tweets analysed so far: 104000\n",
      "Tweets analysed so far: 104500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 734 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets analysed so far: 105000\n",
      "Tweets analysed so far: 105500\n",
      "Tweets analysed so far: 106000\n",
      "Tweets analysed so far: 106500\n",
      "Tweets analysed so far: 107000\n",
      "Tweets analysed so far: 107500\n",
      "Tweets analysed so far: 108000\n",
      "Tweets analysed so far: 108500\n",
      "Tweets analysed so far: 109000\n",
      "Tweets analysed so far: 109500\n",
      "Tweets analysed so far: 110000\n",
      "Tweets analysed so far: 110500\n",
      "Tweets analysed so far: 111000\n",
      "Tweets analysed so far: 111500\n",
      "Tweets analysed so far: 112000\n",
      "Tweets analysed so far: 112500\n",
      "Tweets analysed so far: 113000\n",
      "Tweets analysed so far: 113500\n",
      "Tweets analysed so far: 114000\n",
      "Tweets analysed so far: 114500\n",
      "Tweets analysed so far: 115000\n",
      "Tweets analysed so far: 115500\n",
      "Tweets analysed so far: 116000\n",
      "Tweets analysed so far: 116500\n",
      "Tweets analysed so far: 117000\n",
      "Tweets analysed so far: 117500\n",
      "Tweets analysed so far: 118000\n",
      "Tweets analysed so far: 118500\n",
      "Tweets analysed so far: 119000\n",
      "Tweets analysed so far: 119500\n",
      "Tweets analysed so far: 120000\n",
      "Tweets analysed so far: 120500\n",
      "Tweets analysed so far: 121000\n",
      "Tweets analysed so far: 121500\n",
      "Tweets analysed so far: 122000\n",
      "Tweets analysed so far: 122500\n",
      "Tweets analysed so far: 123000\n",
      "Tweets analysed so far: 123500\n",
      "Tweets analysed so far: 124000\n",
      "Tweets analysed so far: 124500\n",
      "Tweets analysed so far: 125000\n",
      "Tweets analysed so far: 125500\n",
      "Tweets analysed so far: 126000\n",
      "Tweets analysed so far: 126500\n",
      "Tweets analysed so far: 127000\n",
      "Tweets analysed so far: 127500\n",
      "Tweets analysed so far: 128000\n",
      "Tweets analysed so far: 128500\n",
      "Tweets analysed so far: 129000\n",
      "Tweets analysed so far: 129500\n",
      "Tweets analysed so far: 130000\n",
      "Tweets analysed so far: 130500\n",
      "Tweets analysed so far: 131000\n",
      "Tweets analysed so far: 131500\n",
      "Tweets analysed so far: 132000\n",
      "Tweets analysed so far: 132500\n",
      "Tweets analysed so far: 133000\n",
      "Tweets analysed so far: 133500\n",
      "Tweets analysed so far: 134000\n",
      "Tweets analysed so far: 134500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 735 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets analysed so far: 135000\n",
      "Tweets analysed so far: 135500\n",
      "Tweets analysed so far: 136000\n",
      "Tweets analysed so far: 136500\n",
      "Tweets analysed so far: 137000\n",
      "Tweets analysed so far: 137500\n",
      "Tweets analysed so far: 138000\n",
      "Tweets analysed so far: 138500\n",
      "Tweets analysed so far: 139000\n",
      "Tweets analysed so far: 139500\n",
      "Tweets analysed so far: 140000\n",
      "Tweets analysed so far: 140500\n",
      "Tweets analysed so far: 141000\n",
      "Tweets analysed so far: 141500\n",
      "Tweets analysed so far: 142000\n",
      "Tweets analysed so far: 142500\n",
      "Tweets analysed so far: 143000\n",
      "Tweets analysed so far: 143500\n",
      "Tweets analysed so far: 144000\n",
      "Tweets analysed so far: 144500\n",
      "Tweets analysed so far: 145000\n",
      "Tweets analysed so far: 145500\n",
      "Tweets analysed so far: 146000\n",
      "Tweets analysed so far: 146500\n",
      "Tweets analysed so far: 147000\n",
      "Tweets analysed so far: 147500\n",
      "Tweets analysed so far: 148000\n",
      "Tweets analysed so far: 148500\n",
      "Tweets analysed so far: 149000\n",
      "Tweets analysed so far: 149500\n",
      "Tweets analysed so far: 150000\n",
      "Tweets analysed so far: 150500\n",
      "Tweets analysed so far: 151000\n",
      "Tweets analysed so far: 151500\n",
      "Tweets analysed so far: 152000\n",
      "Tweets analysed so far: 152500\n",
      "Tweets analysed so far: 153000\n",
      "Tweets analysed so far: 153500\n",
      "Tweets analysed so far: 154000\n",
      "Tweets analysed so far: 154500\n",
      "Tweets analysed so far: 155000\n",
      "Tweets analysed so far: 155500\n",
      "Tweets analysed so far: 156000\n",
      "Tweets analysed so far: 156500\n",
      "Tweets analysed so far: 157000\n",
      "Tweets analysed so far: 157500\n",
      "Tweets analysed so far: 158000\n",
      "Tweets analysed so far: 158500\n",
      "Tweets analysed so far: 159000\n",
      "Tweets analysed so far: 159500\n",
      "Tweets analysed so far: 160000\n",
      "Tweets analysed so far: 160500\n",
      "Tweets analysed so far: 161000\n",
      "Tweets analysed so far: 161500\n",
      "Tweets analysed so far: 162000\n",
      "Tweets analysed so far: 162500\n",
      "Tweets analysed so far: 163000\n",
      "Tweets analysed so far: 163500\n",
      "Tweets analysed so far: 164000\n",
      "Tweets analysed so far: 164500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit exceeded. Sleeping for 751 seconds.\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='api.twitter.com', port=443): Max retries exceeded with url: /2/tweets?tweet.fields=created_at%2Ctext%2Clang%2Cauthor_id%2Csource&user.fields=location%2Cdescription%2Cverified&expansions=author_id&ids=707804827254923265%2C707805068423200768%2C707805250481233920%2C707805464122298368%2C707805771656986626%2C707805980571074561%2C707805981573517315%2C707806134565015552%2C707806165359529984%2C707806353650262017%2C707807161557098496%2C707807306248024064%2C707807918951899136%2C707808082613706758%2C707808285467013123%2C707808320845914112%2C707808374423994369%2C707808478560124930%2C707808573489815553%2C707809011316428800%2C707809164651790336%2C707809269882732545%2C707809279487647744%2C707809290501885953%2C707809369321316352%2C707809510807769094%2C707809588943429632%2C707809612964167680%2C707809926295523328%2C707811015992479744%2C707811076931522560%2C707811248805703680%2C707811254077882368%2C707811261640261632%2C707811583502712832%2C707811929092444163%2C707812148571979780%2C707812428357214208%2C707812467771109376%2C707812499312287744%2C707812515573571585%2C707813093821251584%2C707813334624686080%2C707813537251520512%2C707813647511396352%2C707813787303350272%2C707814012994588672%2C707817066292781056%2C707824067978645504%2C707833417862230016%2C707833894347927553%2C707835114219122688%2C707835161656868865%2C707839237257691136%2C707839355554013184%2C707850771568599041%2C707851141535690754%2C707852281555714048%2C707864603934400512%2C707865133616201728%2C707865405566656512%2C707866115502792704%2C707883216133169152%2C707888638076407808%2C707889849542389760%2C707892498845323264%2C707902712315305984%2C707916164630974465%2C707918510643609600%2C707923747911524352%2C707924350461026304%2C707939611926994944%2C707955460905836544%2C707967932983173120%2C708000238624251906%2C708000811121442816%2C708004244243107840%2C708015595439202304%2C708080940019486720%2C708081088854274053%2C708150377460269056%2C708150432925917184%2C708388034224324608%2C708400546990977024%2C708735070933274624%2C708970660786208768%2C720787591793098752%2C720824767754801153%2C720840934699638785%2C720841029478256640%2C720841298190606336%2C720841363344855040%2C720841437437235200%2C720841446773751808%2C720841501555560448%2C720841523798016000%2C720841952841740288%2C720842244354220032%2C720842377884094464%2C720842517378301953 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001E88950EAF0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connection.py:169\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m     conn \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mcreate_connection(\n\u001b[0;32m    170\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dns_host, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_kw\n\u001b[0;32m    171\u001b[0m     )\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m six\u001b[38;5;241m.\u001b[39mraise_from(\n\u001b[0;32m     70\u001b[0m         LocationParseError(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, label empty or too long\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m host), \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m     )\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     74\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py:953\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[1;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[0;32m    952\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 953\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    954\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n",
      "\u001b[1;31mgaierror\u001b[0m: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:699\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;66;03m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 699\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;66;03m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    712\u001b[0m \u001b[38;5;66;03m# mess.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:382\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:1010\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(conn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msock\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1010\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn\u001b[38;5;241m.\u001b[39mis_verified:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connection.py:353\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;66;03m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m     conn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m     hostname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connection.py:181\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m e\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conn\n",
      "\u001b[1;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x000001E88950EAF0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\adapters.py:489\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chunked:\n\u001b[1;32m--> 489\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\connectionpool.py:755\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    753\u001b[0m     e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m--> 755\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    758\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\urllib3\\util\\retry.py:574\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[1;32m--> 574\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause))\n\u001b[0;32m    576\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='api.twitter.com', port=443): Max retries exceeded with url: /2/tweets?tweet.fields=created_at%2Ctext%2Clang%2Cauthor_id%2Csource&user.fields=location%2Cdescription%2Cverified&expansions=author_id&ids=707804827254923265%2C707805068423200768%2C707805250481233920%2C707805464122298368%2C707805771656986626%2C707805980571074561%2C707805981573517315%2C707806134565015552%2C707806165359529984%2C707806353650262017%2C707807161557098496%2C707807306248024064%2C707807918951899136%2C707808082613706758%2C707808285467013123%2C707808320845914112%2C707808374423994369%2C707808478560124930%2C707808573489815553%2C707809011316428800%2C707809164651790336%2C707809269882732545%2C707809279487647744%2C707809290501885953%2C707809369321316352%2C707809510807769094%2C707809588943429632%2C707809612964167680%2C707809926295523328%2C707811015992479744%2C707811076931522560%2C707811248805703680%2C707811254077882368%2C707811261640261632%2C707811583502712832%2C707811929092444163%2C707812148571979780%2C707812428357214208%2C707812467771109376%2C707812499312287744%2C707812515573571585%2C707813093821251584%2C707813334624686080%2C707813537251520512%2C707813647511396352%2C707813787303350272%2C707814012994588672%2C707817066292781056%2C707824067978645504%2C707833417862230016%2C707833894347927553%2C707835114219122688%2C707835161656868865%2C707839237257691136%2C707839355554013184%2C707850771568599041%2C707851141535690754%2C707852281555714048%2C707864603934400512%2C707865133616201728%2C707865405566656512%2C707866115502792704%2C707883216133169152%2C707888638076407808%2C707889849542389760%2C707892498845323264%2C707902712315305984%2C707916164630974465%2C707918510643609600%2C707923747911524352%2C707924350461026304%2C707939611926994944%2C707955460905836544%2C707967932983173120%2C708000238624251906%2C708000811121442816%2C708004244243107840%2C708015595439202304%2C708080940019486720%2C708081088854274053%2C708150377460269056%2C708150432925917184%2C708388034224324608%2C708400546990977024%2C708735070933274624%2C708970660786208768%2C720787591793098752%2C720824767754801153%2C720840934699638785%2C720841029478256640%2C720841298190606336%2C720841363344855040%2C720841437437235200%2C720841446773751808%2C720841501555560448%2C720841523798016000%2C720841952841740288%2C720842244354220032%2C720842377884094464%2C720842517378301953 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001E88950EAF0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 83\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_ix \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTweets analysed so far:\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_ix)\n\u001b[1;32m---> 83\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIDs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_ix\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_ix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mtweet_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\n\u001b[0;32m     85\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcreated_at\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlang\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauthor_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m                             \u001b[49m\u001b[43muser_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\n\u001b[0;32m     87\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mverified\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mexpansions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauthor_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# https://stackoverflow.com/questions/38987/how-do-i-merge-two-dictionaries-in-a-single-expression\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m#     tweets_dict = tweets_dict | response.json()\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m#tweets_dict.append(response.json)\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# https://www.kirenz.com/post/2021-12-10-twitter-api-v2-tweepy-and-pandas-in-python/twitter-api-v2-tweepy-and-pandas-in-python/\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Save data as dictionary\u001b[39;00m\n\u001b[0;32m     95\u001b[0m tweets_dict \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson() \n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tweepy\\client.py:1818\u001b[0m, in \u001b[0;36mClient.get_tweets\u001b[1;34m(self, ids, user_auth, **params)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;124;03m\"\"\"get_tweets( \\\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;124;03m    ids, *, expansions=None, media_fields=None, place_fields=None, \\\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;124;03m    poll_fields=None, tweet_fields=None, user_fields=None, \\\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;124;03mhttps://developer.twitter.com/en/docs/twitter-api/tweets/lookup/api-reference/get-tweets\u001b[39;00m\n\u001b[0;32m   1816\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1817\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[1;32m-> 1818\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1819\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/2/tweets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1820\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1821\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexpansions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmedia.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mplace.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1822\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpoll.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtweet.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser.fields\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m   1823\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTweet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_auth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_auth\u001b[49m\n\u001b[0;32m   1824\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tweepy\\client.py:129\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[1;34m(self, method, route, params, endpoint_parameters, json, data_type, user_auth)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m, method, route, params\u001b[38;5;241m=\u001b[39m{}, endpoint_parameters\u001b[38;5;241m=\u001b[39m(), json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m     data_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_auth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    126\u001b[0m ):\n\u001b[0;32m    127\u001b[0m     request_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_params(params, endpoint_parameters)\n\u001b[1;32m--> 129\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_auth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_auth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_type \u001b[38;5;129;01mis\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mResponse:\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tweepy\\client.py:113\u001b[0m, in \u001b[0;36mBaseClient.request\u001b[1;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[0;32m    108\u001b[0m         log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    109\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRate limit exceeded. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSleeping for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msleep_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m         )\n\u001b[0;32m    112\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(sleep_time)\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_auth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequests(response)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tweepy\\client.py:84\u001b[0m, in \u001b[0;36mBaseClient.request\u001b[1;34m(self, method, route, params, json, user_auth)\u001b[0m\n\u001b[0;32m     75\u001b[0m     headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbearer_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking API request: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost \u001b[38;5;241m+\u001b[39m route\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeaders: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheaders\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBody: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     82\u001b[0m )\n\u001b[1;32m---> 84\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhost\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mroute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[0;32m     88\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived API response: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHeaders: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\sessions.py:587\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    582\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    585\u001b[0m }\n\u001b[0;32m    586\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 587\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(prep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msend_kwargs)\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\sessions.py:701\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    698\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    700\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m r \u001b[38;5;241m=\u001b[39m adapter\u001b[38;5;241m.\u001b[39msend(request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    703\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    704\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\adapters.py:565\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e\u001b[38;5;241m.\u001b[39mreason, _SSLError):\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;66;03m# This branch is for urllib3 v1.22 and later.\u001b[39;00m\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "\u001b[1;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='api.twitter.com', port=443): Max retries exceeded with url: /2/tweets?tweet.fields=created_at%2Ctext%2Clang%2Cauthor_id%2Csource&user.fields=location%2Cdescription%2Cverified&expansions=author_id&ids=707804827254923265%2C707805068423200768%2C707805250481233920%2C707805464122298368%2C707805771656986626%2C707805980571074561%2C707805981573517315%2C707806134565015552%2C707806165359529984%2C707806353650262017%2C707807161557098496%2C707807306248024064%2C707807918951899136%2C707808082613706758%2C707808285467013123%2C707808320845914112%2C707808374423994369%2C707808478560124930%2C707808573489815553%2C707809011316428800%2C707809164651790336%2C707809269882732545%2C707809279487647744%2C707809290501885953%2C707809369321316352%2C707809510807769094%2C707809588943429632%2C707809612964167680%2C707809926295523328%2C707811015992479744%2C707811076931522560%2C707811248805703680%2C707811254077882368%2C707811261640261632%2C707811583502712832%2C707811929092444163%2C707812148571979780%2C707812428357214208%2C707812467771109376%2C707812499312287744%2C707812515573571585%2C707813093821251584%2C707813334624686080%2C707813537251520512%2C707813647511396352%2C707813787303350272%2C707814012994588672%2C707817066292781056%2C707824067978645504%2C707833417862230016%2C707833894347927553%2C707835114219122688%2C707835161656868865%2C707839237257691136%2C707839355554013184%2C707850771568599041%2C707851141535690754%2C707852281555714048%2C707864603934400512%2C707865133616201728%2C707865405566656512%2C707866115502792704%2C707883216133169152%2C707888638076407808%2C707889849542389760%2C707892498845323264%2C707902712315305984%2C707916164630974465%2C707918510643609600%2C707923747911524352%2C707924350461026304%2C707939611926994944%2C707955460905836544%2C707967932983173120%2C708000238624251906%2C708000811121442816%2C708004244243107840%2C708015595439202304%2C708080940019486720%2C708081088854274053%2C708150377460269056%2C708150432925917184%2C708388034224324608%2C708400546990977024%2C708735070933274624%2C708970660786208768%2C720787591793098752%2C720824767754801153%2C720840934699638785%2C720841029478256640%2C720841298190606336%2C720841363344855040%2C720841437437235200%2C720841446773751808%2C720841501555560448%2C720841523798016000%2C720841952841740288%2C720842244354220032%2C720842377884094464%2C720842517378301953 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000001E88950EAF0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed'))"
     ]
    }
   ],
   "source": [
    "#print(\"type of response:\", type(response))\n",
    "# print(response)\n",
    "# print(len(response.includes))\n",
    "#print(response)\n",
    "#print(response.includes['users'])\n",
    "# counter = 0\n",
    "# last_author_id = 0\n",
    "# for tweet in response.data:\n",
    "#     # \"user.location\", \"user.followers_count\", \"user.friends_count\",\n",
    "#     #                          \"user.favourites_count\", \"user.description\", \"user.verified\"\n",
    "#     #print(tweet)\n",
    "#     if last_author_id == tweet.author_id:\n",
    "#         counter -= 1\n",
    "#     print(tweet.id, tweet.created_at, tweet.author_id)\n",
    "#     print(response.includes['users'][counter].id)\n",
    "#     last_author_id = tweet.author_id\n",
    "#     counter += 1\n",
    "\n",
    "import requests\n",
    "\n",
    "client = tweepy.Client(bearer_token,\n",
    "                       return_type = requests.Response,\n",
    "                       wait_on_rate_limit=True)\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------- IMPORTANT!! (Tweet IDs)\n",
    "print(\"Fake news!\")\n",
    "IDs = pf_fake_tweet_ids\n",
    "n = len(IDs)\n",
    "# tweets_dict = {}\n",
    "for batch_ix in range(0, n, 100):\n",
    "    if batch_ix % 500 == 0:\n",
    "        print(\"Tweets analysed so far:\", batch_ix)\n",
    "    \n",
    "    response = client.get_tweets(IDs[batch_ix : min(batch_ix + 100, n)], \n",
    "                                 tweet_fields=\n",
    "                                     [\"created_at\", \"text\", \"lang\", \"author_id\", \"source\"],\n",
    "                                 user_fields=\n",
    "                                     [\"location\", \"description\", \"verified\"],\n",
    "                                 expansions=\"author_id\")\n",
    "    # https://stackoverflow.com/questions/38987/how-do-i-merge-two-dictionaries-in-a-single-expression\n",
    "    #     tweets_dict = tweets_dict | response.json()\n",
    "    #tweets_dict.append(response.json)\n",
    "\n",
    "    # https://www.kirenz.com/post/2021-12-10-twitter-api-v2-tweepy-and-pandas-in-python/twitter-api-v2-tweepy-and-pandas-in-python/\n",
    "    # Save data as dictionary\n",
    "    tweets_dict = response.json() \n",
    "\n",
    "    try:\n",
    "        # Extract \"data\" value from dictionary\n",
    "        tweets_data = tweets_dict['data'] # can fail if no data in response\n",
    "        user_data = tweets_dict['includes']['users']\n",
    "\n",
    "        # Transform to pandas Dataframe\n",
    "        df_tweets = pd.json_normalize(tweets_data)\n",
    "        df_tweets.head()\n",
    "\n",
    "        # Transform to pandas Dataframe\n",
    "        df_users = pd.json_normalize(user_data)\n",
    "        df_users = df_users.rename(columns={'id': 'author_id'})\n",
    "        df_users.head()\n",
    "\n",
    "        df = pd.merge(df_tweets, df_users, on=[\"author_id\"])\n",
    "\n",
    "        # Standardise order of columns\n",
    "        df = df[['id', 'text', 'author_id', 'source', 'created_at', 'edit_history_tweet_ids', 'lang', 'description', 'username', 'verified', 'name', 'location']]\n",
    "\n",
    "        include_header = batch_ix == 0\n",
    "        df.to_csv(\"fake.csv\", mode='a', header=include_header)\n",
    "    except:\n",
    "        continue\n",
    "    \n",
    "print(\"------------------------------------------------\")\n",
    "print(\"Real news!\")\n",
    "IDs = pf_real_tweet_ids\n",
    "n = len(IDs)\n",
    "# tweets_dict = {}\n",
    "for batch_ix in range(0, n, 100):\n",
    "    if batch_ix % 500 == 0:\n",
    "        print(\"Tweets analysed so far:\", batch_ix)\n",
    "    \n",
    "    response = client.get_tweets(IDs[batch_ix : min(batch_ix + 100, n)], \n",
    "                                 tweet_fields=\n",
    "                                     [\"created_at\", \"text\", \"lang\", \"author_id\", \"source\"],\n",
    "                                 user_fields=\n",
    "                                     [\"location\", \"description\", \"verified\"],\n",
    "                                 expansions=\"author_id\")\n",
    "    # https://stackoverflow.com/questions/38987/how-do-i-merge-two-dictionaries-in-a-single-expression\n",
    "    #     tweets_dict = tweets_dict | response.json()\n",
    "    #tweets_dict.append(response.json)\n",
    "\n",
    "    # https://www.kirenz.com/post/2021-12-10-twitter-api-v2-tweepy-and-pandas-in-python/twitter-api-v2-tweepy-and-pandas-in-python/\n",
    "    # Save data as dictionary\n",
    "    tweets_dict = response.json() \n",
    "\n",
    "    # Extract \"data\" value from dictionary\n",
    "    try:\n",
    "        tweets_data = tweets_dict['data'] # can fail if no data in response\n",
    "        user_data = tweets_dict['includes']['users']\n",
    "\n",
    "        # Transform to pandas Dataframe\n",
    "        df_tweets = pd.json_normalize(tweets_data)\n",
    "        df_tweets.head()\n",
    "\n",
    "        # Transform to pandas Dataframe\n",
    "        df_users = pd.json_normalize(user_data)\n",
    "        df_users = df_users.rename(columns={'id': 'author_id'})\n",
    "        df_users.head()\n",
    "\n",
    "        df = pd.merge(df_tweets, df_users, on=[\"author_id\"])\n",
    "\n",
    "        # Standardise order of columns\n",
    "        df = df[['id', 'text', 'author_id', 'source', 'created_at', 'edit_history_tweet_ids', 'lang', 'description', 'username', 'verified', 'name', 'location']]\n",
    "\n",
    "        include_header = batch_ix == 0\n",
    "        df.to_csv(\"real.csv\", mode='a', header=include_header)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "60f9a8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Created at</th>\n",
       "      <th>User location</th>\n",
       "      <th>Num Followers</th>\n",
       "      <th>Num Friends</th>\n",
       "      <th>Num Favourites</th>\n",
       "      <th>User description</th>\n",
       "      <th>User verified</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>937349434668498944</td>\n",
       "      <td>BREAKING: First NFL Team Declares Bankruptcy O...</td>\n",
       "      <td>15:54:54+00:00</td>\n",
       "      <td>Sugar Land, TX</td>\n",
       "      <td>1712</td>\n",
       "      <td>2751</td>\n",
       "      <td>22141</td>\n",
       "      <td>Ofelia. Arizmendez @ deplorable me. I am a pro...</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>937379378006282240</td>\n",
       "      <td>BREAKING: First NFL Team Declares Bankruptcy O...</td>\n",
       "      <td>17:53:54+00:00</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>99</td>\n",
       "      <td>125</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>937380068590055425</td>\n",
       "      <td>BREAKING: First NFL Team Declares Bankruptcy O...</td>\n",
       "      <td>17:56:38+00:00</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>99</td>\n",
       "      <td>125</td>\n",
       "      <td></td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>937429898670600192</td>\n",
       "      <td>BREAKING: First NFL Team Declares Bankruptcy O...</td>\n",
       "      <td>21:14:39+00:00</td>\n",
       "      <td>Ozarks. Missouri</td>\n",
       "      <td>1162</td>\n",
       "      <td>1258</td>\n",
       "      <td>9422</td>\n",
       "      <td>Happily married conservative Pentecostal woman...</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>937449906352152576</td>\n",
       "      <td>BREAKING: First NFL Team Declares Bankruptcy O...</td>\n",
       "      <td>22:34:09+00:00</td>\n",
       "      <td></td>\n",
       "      <td>12772</td>\n",
       "      <td>13708</td>\n",
       "      <td>23094</td>\n",
       "      <td>Deplorable member of The Silenced Majority.. #...</td>\n",
       "      <td>False</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                               Text  \\\n",
       "0  937349434668498944  BREAKING: First NFL Team Declares Bankruptcy O...   \n",
       "1  937379378006282240  BREAKING: First NFL Team Declares Bankruptcy O...   \n",
       "2  937380068590055425  BREAKING: First NFL Team Declares Bankruptcy O...   \n",
       "3  937429898670600192  BREAKING: First NFL Team Declares Bankruptcy O...   \n",
       "4  937449906352152576  BREAKING: First NFL Team Declares Bankruptcy O...   \n",
       "\n",
       "       Created at      User location  Num Followers  Num Friends  \\\n",
       "0  15:54:54+00:00     Sugar Land, TX           1712         2751   \n",
       "1  17:53:54+00:00                                14           99   \n",
       "2  17:56:38+00:00                                14           99   \n",
       "3  21:14:39+00:00  Ozarks. Missouri            1162         1258   \n",
       "4  22:34:09+00:00                             12772        13708   \n",
       "\n",
       "   Num Favourites                                   User description  \\\n",
       "0           22141  Ofelia. Arizmendez @ deplorable me. I am a pro...   \n",
       "1             125                                                      \n",
       "2             125                                                      \n",
       "3            9422  Happily married conservative Pentecostal woman...   \n",
       "4           23094  Deplorable member of The Silenced Majority.. #...   \n",
       "\n",
       "   User verified Language  \n",
       "0          False       en  \n",
       "1          False       en  \n",
       "2          False       en  \n",
       "3          False       en  \n",
       "4          False       en  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_date(row):\n",
    "    return row['Created at'].timetz()\n",
    "\n",
    "tweets_df['Created at'] = tweets_df.apply(lambda row: remove_date(row), axis=1)\n",
    "\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37c995f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "tweets_df2.to_csv(\"final_short.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
