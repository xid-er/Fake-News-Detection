{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b27bf5b",
   "metadata": {},
   "source": [
    "## PHEME Dataset (330 Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec3b1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_src_text(row):\n",
    "    path = \"data/en/\" + row['event'] + \"/\" + str(row['threadid']) + \"/source-tweets/\" + str(row['tweetid']) + \".json\"\n",
    "    with open(path, \"r\") as f:\n",
    "        source = json.loads(f.read())\n",
    "        return source['text']\n",
    "\n",
    "def is_true(row):\n",
    "    print(row)\n",
    "    path = \"data/en/\" + row['event'] + \"/\" + str(row['threadid']) + \"/annotation.json\"\n",
    "    with open(path, \"r\") as f:\n",
    "        source = json.loads(f.read())\n",
    "        return str(source.get('true', 'unverified'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03307b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset used: https://figshare.com/articles/dataset/PHEME_rumour_scheme_dataset_journalism_use_case/2068650\n",
    "df = pd.read_json(\"data/en-scheme-annotations.json\", dtype = {\"threadid\": str, \"tweetid\": str}, lines=True)\n",
    "\n",
    "df['true'] = df.apply(lambda row: is_true(row), axis=1)\n",
    "df['src_text'] = df.apply(lambda row: get_src_text(row), axis=1)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8222ae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_src_text_by_index(row):\n",
    "    return df.iloc[row.index]['src_text']\n",
    "\n",
    "accuracy = pd.DataFrame([test_labels, lrtfidf_test])\n",
    "print(accuracy)\n",
    "print(lrtfidf_test)\n",
    "print(test_labels)\n",
    "print(type(test_labels))\n",
    "test_df = pd.DataFrame(test_labels)\n",
    "test_df['predicted'] = lrtfidf_test\n",
    "print(test_df.apply(lambda row: get_src_text_by_index(row)))\n",
    "print(test_df)\n",
    "#test_df = pd.DataFrame({\"tweet_id\":test_labels[0], \"actual_label\": test_labels[1]})\n",
    "test_labels.add(lrtfidf_test)\n",
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2d6541",
   "metadata": {},
   "source": [
    "## Experimenting with sample of whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658666e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shorter = train_features[:50, :50]\n",
    "shorter_labels = train_labels[:50]\n",
    "print(train_features[100, 666])\n",
    "#print(train_features)\n",
    "print(train_features.shape)\n",
    "print(shorter.shape)\n",
    "\n",
    "# SVC\n",
    "svc = SVC(kernel='rbf')\n",
    "svc_model = svc.fit(shorter, shorter_labels)\n",
    "svc_test = svc_model.predict(test_features)\n",
    "evaluation_summary(\"SVC test\", test_labels, svc_test)\n",
    "\n",
    "# Logistic Regression with TF-IDF\n",
    "lr_tfidf = LogisticRegression()\n",
    "lr_model_tfidf = lr_tfidf.fit(shorter, shorter_labels)\n",
    "lrtfidf_test = lr_model_tfidf.predict(test_features)\n",
    "evaluation_summary(\"LR (TF-IDF) test\", test_labels, lrtfidf_test)\n",
    "\n",
    "# Dummy Majority\n",
    "dumb = DummyClassifier(strategy='most_frequent')\n",
    "dumb.fit(shorter, shorter_labels)\n",
    "dumb_test = dumb.predict(test_features)\n",
    "evaluation_summary(\"Dummy MF test\", test_labels, dumb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573022a4",
   "metadata": {},
   "source": [
    "## Emoji tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aa3e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_spacymoji = spacy.load(\"en_core_web_sm\")\n",
    "emoji = Emoji(nlp_spacymoji, merge_spans=True)\n",
    "nlp_spacymoji.add_pipe('emoji', first=True)\n",
    "# tokenised_train = train_frame.apply(lambda row: nlp_spacymoji(row['src_text']))\n",
    "# #tok_train = nlp_spacymoji(train_text)\n",
    "# print(tokenised_train)\n",
    "\n",
    "def spacy_tokeniser(text):\n",
    "    tokens = []\n",
    "    for w in nlp_spacymoji(text):\n",
    "        tokens.append(w.lemma_.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b316c96f",
   "metadata": {},
   "source": [
    "## Train-Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06912400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(text_col, label_col):\n",
    "  train_text, temp_text, train_labels, temp_labels = train_test_split(df[text_col], df[label_col], \n",
    "                                                                      random_state=2018, \n",
    "                                                                      test_size=0.3)\n",
    "\n",
    "\n",
    "  val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                  random_state=2018, \n",
    "                                                                  test_size=0.5, \n",
    "                                                                  stratify=temp_labels)\n",
    "\n",
    "  # train_frame = pd.DataFrame([train_text, train_labels])\n",
    "  # val_frame = pd.DataFrame([val_text, val_labels])\n",
    "  # test_frame = pd.DataFrame([test_text, test_labels])\n",
    "  return train_text, train_labels, val_text, val_labels, test_text, test_labels\n",
    "\n",
    "train_text, train_labels, val_text, val_labels, test_text, test_labels = split_dataset('text', 'true')\n",
    "\n",
    "print(type(train_text))\n",
    "print(train_labels.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8d9c63",
   "metadata": {},
   "source": [
    "## Addition of article (incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aadc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimentation - not needed\n",
    "prev_article = \"\"\n",
    "article_counter = 0\n",
    "for row_ix, row in real.iterrows():\n",
    "    if row['article_id'] != prev_article:\n",
    "        print(f\"New article at row index {row_ix}!\")\n",
    "        article_counter += 1\n",
    "        print(\"Articles so far:\", article_counter)\n",
    "        prev_article = row['article_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1c35e3",
   "metadata": {},
   "source": [
    "## Time Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa4ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "\n",
    "def remove_date(row):\n",
    "    #obj = datetime.strptime(row['created_at'], )\n",
    "    return row['created_at'].timetz()\n",
    "\n",
    "def get_hour(row):\n",
    "    return row['created_at'].hour\n",
    "\n",
    "def get_tz(row):\n",
    "    time = row['created_at'].split('T')[1]\n",
    "    time_split = re.split(r'[:|.]', time)\n",
    "    return time_split[-1]\n",
    "\n",
    "df = X.copy()\n",
    "df['created_at'] = pd.to_datetime(df['created_at']) # str to datetime\n",
    "# TODO: Put into section when importing dataset\n",
    "df['created_at'] = df.apply(lambda row: row['created_at'].hour, axis=1)\n",
    "\n",
    "#df.head()\n",
    "df['created_at'].value_counts()\n",
    "\n",
    "import re\n",
    "\n",
    "def stringify_time(row):\n",
    "    time = row['created_at'].split('T')[1]\n",
    "    time_split = re.split(r'[:|.]', time)\n",
    "    return \" \".join(time_split)\n",
    "\n",
    "df = X.copy()\n",
    "df['created_at'] = df.apply(lambda row: stringify_time(row), axis=1)\n",
    "df.head()\n",
    "\n",
    "import pytz\n",
    "\n",
    "df = X\n",
    "#print(df)\n",
    "df['created_at'] = df.apply(lambda row: row['created_at'].astimezone('Europe/Riga'), axis=1)\n",
    "df.head()\n",
    "\n",
    "df.iloc[0]['created_at']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74578fa",
   "metadata": {},
   "source": [
    "## Cleaning database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fcf52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_dataset(X)\n",
    "df['text'] = df.text.apply(nltk_preprocess)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4b1717",
   "metadata": {},
   "source": [
    "## Multi-Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d628cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/georgian-io/Multimodal-Toolkit/blob/master/multimodal_transformers/model/layer_utils.py\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"mlp can specify number of hidden layers and hidden layer channels\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, act='relu', num_hidden_lyr=2,\n",
    "                 dropout_prob=0.5, return_layer_outs=False,\n",
    "                 hidden_channels=None, bn=False):\n",
    "        super().__init__()\n",
    "        self.out_dim = output_dim\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.return_layer_outs = return_layer_outs\n",
    "        if not hidden_channels:\n",
    "            hidden_channels = [input_dim for _ in range(num_hidden_lyr)]\n",
    "        elif len(hidden_channels) != num_hidden_lyr:\n",
    "            raise ValueError(\n",
    "                \"number of hidden layers should be the same as the lengh of hidden_channels\")\n",
    "        self.layer_channels = [input_dim] + hidden_channels + [output_dim]\n",
    "        self.act_name = act\n",
    "        self.activation = create_act(act)\n",
    "        self.layers = nn.ModuleList(list(\n",
    "            map(self.weight_init, [nn.Linear(self.layer_channels[i], self.layer_channels[i + 1])\n",
    "                                   for i in range(len(self.layer_channels) - 2)])))\n",
    "        final_layer = nn.Linear(self.layer_channels[-2], self.layer_channels[-1])\n",
    "        self.weight_init(final_layer,   activation='linear')\n",
    "        self.layers.append(final_layer)\n",
    "\n",
    "        self.bn = bn\n",
    "        if self.bn:\n",
    "            self.bn = nn.ModuleList([torch.nn.BatchNorm1d(dim) for dim in self.layer_channels[1:-1]])\n",
    "\n",
    "    def weight_init(self, m, activation=None):\n",
    "        if activation is None:\n",
    "            activation = self.act_name\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain=nn.init.calculate_gain(activation))\n",
    "        return m\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: the input features\n",
    "        :return: tuple containing output of MLP,\n",
    "                and list of inputs and outputs at every layer\n",
    "        \"\"\"\n",
    "        layer_inputs = [x]\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            input = layer_inputs[-1]\n",
    "            if layer == self.layers[-1]:\n",
    "                layer_inputs.append(layer(input))\n",
    "            else:\n",
    "                if self.bn:\n",
    "                    output = self.activation(self.bn[i](layer(input)))\n",
    "                else:\n",
    "                    output = self.activation(layer(input))\n",
    "                layer_inputs.append(self.dropout(output))\n",
    "\n",
    "        # model.store_layer_output(self, layer_inputs[-1])\n",
    "        if self.return_layer_outs:\n",
    "            return layer_inputs[-1], layer_inputs\n",
    "        else:\n",
    "            return layer_inputs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843b634c",
   "metadata": {},
   "source": [
    "## Concatenate Features (Multimodal Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b41f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pipe['encoder'].get_feature_names_out())\n",
    "\n",
    "# Define categorical columns\n",
    "categorical = ['verified', 'created_at']\n",
    "print(f\"Categorical columns are: {categorical}\")\n",
    "\n",
    "lexical = ['description', 'location', 'name', 'source', 'text', 'username']\n",
    "print(f\"Lexical columns are: {lexical}\")\n",
    "\n",
    "df = X.copy()\n",
    "for col_name in categorical:\n",
    "    df[col_name] = df[col_name].astype('category')\n",
    "    df[col_name] = df[col_name].cat.codes.astype('float')\n",
    "    \n",
    "df[categorical].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58789b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_combined = []\n",
    "for (i, row) in df.iterrows():\n",
    "    combined = ' [SEP] '.join([str(row[text_col]) for text_col in lexical])\n",
    "    text_combined.append(combined)\n",
    "    \n",
    "df.insert(0, 'all_text', text_combined)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151d5b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump(tfidf, open(\"tfidf_vectorizer.sav\", \"wb\"))\n",
    "#print(train_text)\n",
    "# Fit pipeline to training data\n",
    "\n",
    "# https://towardsdatascience.com/pipeline-columntransformer-and-featureunion-explained-f5491f815f\n",
    "# Interesting read for time-related feature engineering: \n",
    "#   https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html\n",
    "\n",
    "# class TextProcessor(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self):\n",
    "#         self.text_transformer = Pipeline([\n",
    "#             ('tfidf_vectoriser', TfidfVectorizer()),\n",
    "#         ])\n",
    "       \n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X, y=None):\n",
    "#         return self.text_transformer.fit_transform(X.squeeze()).toarray()\n",
    "\n",
    "# class CategoryProcessor(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self):\n",
    "#         self.text_transformer = Pipeline([\n",
    "#             ('1hot_encoder', OneHotEncoder(handle_unknown='ignore', sparse=False)),\n",
    "#         ])\n",
    "       \n",
    "#     def fit(self, X, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X, y=None):\n",
    "#         return self.text_transformer.fit_transform(X.squeeze()).toarray()\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "    \n",
    "def get_transformers(keys):\n",
    "    transformers = []\n",
    "    for key in keys:\n",
    "        transformers.append(\n",
    "            (key, Pipeline([\n",
    "                ('selector', ItemSelector(key=key)),\n",
    "                ('tfidf', TfidfVectorizer())\n",
    "            ]))\n",
    "        )\n",
    "    return transformers\n",
    "    \n",
    "cat_pipe = Pipeline([\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "lex_pipe = Pipeline([\n",
    "    #(\"squeeze\", FunctionTransformer(lambda x: x.squeeze())),\n",
    "    ('encoder', TfidfVectorizer()),\n",
    "    #(\"toarray\", FunctionTransformer(lambda x: x.toarray()))\n",
    "])\n",
    "\n",
    "lex_pipe = FeatureUnion([\n",
    "        ('text', Pipeline([\n",
    "            ('selector', ItemSelector(key='text')),\n",
    "            ('tfidf', TfidfVectorizer())])),\n",
    "#         ('desc', Pipeline([\n",
    "#             ('selector', ItemSelector(key='description')),\n",
    "#             ('tfidf', TfidfVectorizer())])),\n",
    "#         ('loc', Pipeline([\n",
    "#             ('selector', ItemSelector(key='location')),\n",
    "#             ('tfidf', TfidfVectorizer())])),\n",
    "#         ('name', Pipeline([\n",
    "#             ('selector', ItemSelector(key='name')),\n",
    "#             ('tfidf', TfidfVectorizer())])),\n",
    "#         ('source', Pipeline([\n",
    "#             ('selector', ItemSelector(key='source')),\n",
    "#             ('tfidf', TfidfVectorizer())])),\n",
    "#         ('user', Pipeline([\n",
    "#             ('selector', ItemSelector(key='username')),\n",
    "#             ('tfidf', TfidfVectorizer())])),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('lex', lex_pipe, lexical),\n",
    "    ('cat', cat_pipe, categorical),\n",
    "])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LogisticRegression(warm_start=True))\n",
    "    #('model', SGDClassifier(loss='log_loss'))\n",
    "])\n",
    "\n",
    "print(X_train.shape)\n",
    "\n",
    "mini_model = SGDClassifier(loss='log_loss')\n",
    "\n",
    "batch_size = 1000\n",
    "for batch_ix in range(0, len(X_train), batch_size):\n",
    "    ix = min(batch_ix + batch_size, len(X_train))\n",
    "    #pipe.fit(X_train.iloc[batch_ix:ix, :], y_train.iloc[batch_ix:ix])\n",
    "    mini_model.partial_fit(X_train.iloc[batch_ix:ix, :], y_train.iloc[batch_ix:ix])\n",
    "\n",
    "#pipe.fit(X_train, y_train) # hahaha, naive thinking this would work\n",
    "\n",
    "# Predict test data\n",
    "#y_test_pred = pipe.predict(X_test)\n",
    "y_test_pred = mini_model.predict(X_test)\n",
    "\n",
    "\n",
    "evaluation_summary(\"LR multi-feature test\", y_test, t_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=NbbsVcs42jE\n",
    "\n",
    "from torch import nn\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "class BertConcatFeatures(BertForSequenceClassification):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config) # BERT setup\n",
    "        \n",
    "        self.num_labels = config.num_labels\n",
    "        \n",
    "        combined_feat_dim = config.text_feat_dim + \\\n",
    "                            config.cat_feat_dim + \\\n",
    "                            config.numerical_feat_dim\n",
    "        \n",
    "        self.num_bn = nn.BatchNorm1d(config.numerical_feat_dim)\n",
    "        \n",
    "        dims = []\n",
    "        dim = combined_feat_dim\n",
    "        while True:\n",
    "            dim = dim // 4\n",
    "            if dim <= self.num_labels:\n",
    "                break \n",
    "            dim.append(int(dim))\n",
    "        print('MLP layer sizes:')\n",
    "        print('   Input:', combined_feat_dim)\n",
    "        print('   Hidden:', dims)\n",
    "        print('   Output:', self.num_labels)\n",
    "        print('')\n",
    "        \n",
    "        self.mlp = MLP(combined_feat_dim,\n",
    "                       self.num_labels,\n",
    "                       num_hidden_layers=len(dims),\n",
    "                       dropout_prob=0.1,\n",
    "                       hidden_channels=dims,\n",
    "                       bn=True)\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None,\n",
    "                position_ids=None, head_mask=None, inputs_embeds=None, labels=None,\n",
    "                class_weights=None, output_attentions=None, output_hidden_states=None,\n",
    "                cat_feats=None, numerical_feats=None):\n",
    "        \n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            position_ids=position_ids,\n",
    "                            head_mask=head_mask,\n",
    "                            inputs_embeds=inputs_embeds,\n",
    "                            output_attentions=output_attentions,\n",
    "                            output_hidden_states=output_hidden_states\n",
    "                           )\n",
    "        \n",
    "        # outputs[0] - ALL output embeddings from BERT\n",
    "        # outputs[1] - [CLS] token embedding, with additional \"pooling\" done\n",
    "        cls = outputs[1]\n",
    "        cls = self.dropout(cls)\n",
    "        \n",
    "        # Concatenate features\n",
    "        \n",
    "        numerical_feats = self.num_bn(numerical_feats) # batch-normalise numerical features\n",
    "        combined_feats = torch.cat((cls, cat_feats, numerical_feats), dim=1)\n",
    "        \n",
    "        # Output classifier / Regression\n",
    "        \n",
    "        logits = self.mlp(combined_feats)\n",
    "        \n",
    "        if type(logits) is tuple: # regression\n",
    "            logits, classifier_layer_outputs = logits[0], logits[1]\n",
    "        else: # simple classifier\n",
    "            classifier_layer_outputs = [combined_feats, logits]\n",
    "            \n",
    "        # Loss\n",
    "        if labels is not None:\n",
    "            # Regression\n",
    "            if self.num_labels == 1:\n",
    "                loss_fct = MSELoss()\n",
    "                labels = labels.float()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            # Classification\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss(weight=class_weights)\n",
    "                labels = labels.long()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "                \n",
    "        else:\n",
    "            loss = None\n",
    "            \n",
    "        results = {\n",
    "            'loss': loss,\n",
    "            'logits': logits,\n",
    "            'classifier_layer_outputs': classifier_layer_outputs\n",
    "        }\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d933b4",
   "metadata": {},
   "source": [
    "## Old BERT Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93167010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb#scrollTo=URn-DWJt5xhP\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "import transformers as ppb\n",
    "model_class, tokenizer_class, bert_model_name = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(bert_model_name)\n",
    "model = model_class.from_pretrained(bert_model_name)\n",
    "\n",
    "tokenized = df['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "\n",
    "padded_encoding = df['text'].apply(\n",
    "    lambda src_text: tokenizer.encode_plus(\n",
    "        src_text, add_special_tokens = True,\n",
    "        truncation = True, padding = \"max_length\", \n",
    "        return_attention_mask = True, return_tensors = \"pt\")\n",
    ")\n",
    "\n",
    "#input_ids = torch.tensor(np.array(padded_encoding))\n",
    "\n",
    "# print(padded_encoding.shape)\n",
    "\n",
    "# #new_tensor = torch.tensor([200, padded_encoding])\n",
    "\n",
    "new_tensor = padded_encoding[None, :]\n",
    "#print(new_tensor)\n",
    "new_tensor[0] = 200\n",
    "#print(new_tensor.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(new_tensor) # ERROR: Tries to get 255 GB of RAM ============================================\n",
    "    # The above line is the one responsible for abandoning this approach because this BERT implementation could not be \n",
    "    # applied in batches\n",
    "    \n",
    "    \n",
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "\n",
    "labels = df['true']\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)\n",
    "\n",
    "lr_clf.score(test_features, test_labels) # 0.5866666666666667 score for PHEME dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
