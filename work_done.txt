Week 3: 03/10/22 - 06/10/22

Created and shared Trello board to manage tasks.
Found a climate change-focused dataset with ternary classification: Supports, Not Enough Info, Refutes.
Found a set of Facebook- and Twitter-only datasets, wanted to discuss in meeting which one is more appropriate.
* Credbank: Twitter, ~60 million posts, 5 levels of truth  (less trustworthy)
* PHEME: Twitter, 330 posts, binary classification (true/fake) ---- (same-length posts)
* BuzzFace: Facebook, 2263 posts, 4 levels of truth
Researched tutorials to follow for classification; one is training overnight, but seems like the model environment will take the least time for the project, which allows for focusing on more novel parts, like emoji-translation (not yet researched) and the end-user tool (Django website/Browser extension (NB: to discuss) analysing Twitter/Facebook/Text).


Week 4: 10/10/22 - 13/10/22

Analysed the smaller Twitter dataset much further by reading the connected article (https://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0150989) and after (much difficulty with) importing the dataset into a Python data structure.
TODO: tokenise data.
Vectorised dataset with TF-IDF vectoriser.
Created 3 ML classifier models with the dataset and compared their results:
* Dummy Classifier with the most-frequent strategy: weighted f1-score - 22.9%
* SVM/SVC Classifier: weighted f1-score - 58.9%
* Logistic Regression Classifier: weighted f1-score - 61.2%

Reminder that the code is in the repo: https://github.com/xid-er/Fake-News-Detection


Week 5: 17/10/22 - 21/10/22

Experimented with emoji tokenisation and found that performance for all models was worse: there aren't any emojis in the 300-post dataset, and maybe the imported tokeniser is not as good as the in-built one
Researched how to use BERT to prepare the data: fully integrated into the code, but the performance was slightly worse than Logistic Regression before. I even used the full BERT instead of the light version - distilBERT. How come?
Improved dataset cleaning according to tutorials (this might be the reason BERT did worse - information lost?)